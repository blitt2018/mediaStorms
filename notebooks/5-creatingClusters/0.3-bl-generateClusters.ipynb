{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file...\n",
    "1. finds the cosine similarity between articles with at least 1 named entity \n",
    "2. filters down to only article pairs closer than 1 week \n",
    "3. creates a graph of articles with cosine sim. weighted edges\n",
    "4. outputs connected components of that graph after considering only weighted edges above a certain \n",
    "similarity threshold\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial import distance \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: very important, which entity categories to keep \n",
    "#article showing all entity types below\n",
    "# https://www.kaggle.com/code/curiousprogrammer/entity-extraction-and-classification-using-spacy\n",
    "TO_KEEP = [\"org\",\"event\", \"person\", \"work_of_art\", \"product\"]\n",
    "\n",
    "#only keep named entities that have a # of articles associated with them in this range \n",
    "CLUSTER_CUTOFF = [2, 20000]\n",
    "\n",
    "#this is the lowest cosine similarity threshold we will use \n",
    "SIM_THRESH = .8\n",
    "\n",
    "INVERTED_ENT_PATH = \"/shared/3/projects/newsDiffusion/data/interim/NEREmbedding/invertedEntityIndex.pkl\"\n",
    "EMBEDS_PATH = \"/shared/3/projects/newsDiffusion/data/processed/articleEmbeddings/embeddings.pkl\"\n",
    "CLEANED_DF_PATH = \"/shared/3/projects/newsDiffusion/data/processed/newsData/fullDataWithNERCleaned.tsv\"\n",
    "\n",
    "#how many days apart do we allow our articles to be? \n",
    "#if we consider adding edge between them based on cosine similarity \n",
    "#7 allows for Sunday - Sunday but NOT Sunday - Monday of following week \n",
    "DATE_FILTER = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the df with our inverted index in it\n",
    "invertedDf = pd.read_pickle(INVERTED_ENT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what are the entities with the top number of associated articles \n",
    "invertedDf.sort_values(\"numArticles\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before filtering: 7436473\n",
      "len after filtering: 2614176\n"
     ]
    }
   ],
   "source": [
    "print(f\"len before filtering: {len(invertedDf)}\")\n",
    "invertedDf = invertedDf[(invertedDf[\"numArticles\"] >= CLUSTER_CUTOFF[0]) & (invertedDf[\"numArticles\"] <= CLUSTER_CUTOFF[1])]\n",
    "print(f\"len after filtering: {len(invertedDf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19817"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(invertedDf[\"numArticles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a dictionary so we can get the embeddings we need quickly \n",
    "#embeddingsDict = embeddingsDf.set_index(\"key\").to_dict(orient=\"index\")\n",
    "embedsFile = open(EMBEDS_PATH, \"rb\")\n",
    "embeddingsDict = pickle.load(embedsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of the keys that correspond to each named entity \n",
    "#sort so that smaller clusters will be processed first :) \n",
    "keyOptions = list(invertedDf.sort_values(\"numArticles\", ascending=False)[\"key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2614176/2614176 [43:32<00:00, 1000.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#very simple, go through each list of articles associated with our named entities\n",
    "#next, get cosine similarity, and store only those articles where cosine sim is > .8 \n",
    "\n",
    "simList = []\n",
    "lKeyList = []\n",
    "rKeyList = []\n",
    "\n",
    "#for each list of article keys associated with entities \n",
    "for i, entGroup in enumerate(tqdm(keyOptions)): \n",
    "    \n",
    "    #within each list of article keys, consider the unique pairs \n",
    "    #and get their cosine similarities \n",
    "    #for i in range(0, len(entGroup)): \n",
    "    myMat = np.matrix([embeddingsDict[key][\"embedding\"] for key in entGroup])\n",
    "    pairSims = cosine_similarity(myMat).flatten()\n",
    "    entGroup = np.array(entGroup)\n",
    "    \n",
    "    greaterThan = np.where(pairSims > .8)\n",
    "    simList.append(pairSims[greaterThan])\n",
    "    \n",
    "    #get the equivalent row and column indices to what we have in the flattened array \n",
    "    left = [math.floor(index / len(entGroup)) for index in greaterThan[0]]\n",
    "    right = [index % len(entGroup) for index in greaterThan[0]]\n",
    "    \n",
    "    #get the keys corresponding to the elements we selected from the list \n",
    "    lKeyList.append(entGroup[left])\n",
    "    rKeyList.append(entGroup[right])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all of the lists of np arrays we made \n",
    "simCat = np.concatenate(simList)\n",
    "lKeyCat = np.concatenate(lKeyList)\n",
    "rKeyCat = np.concatenate(rKeyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dataframe gives us the article pair (left,right keys) and the similarity for that pair\n",
    "#in long format \n",
    "pairsDf = pd.DataFrame({\"lKey\":lKeyCat, \"rKey\":rKeyCat,\"simScore\":simCat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick sanity check\n",
    "#does a random pair have the same similarity as if we recompute it directly \n",
    "randIndex = 56566740\n",
    "randRow = pairsDf.loc[randIndex]\n",
    "\n",
    "1 - cosine(embeddingsDict[randRow[\"lKey\"]][\"embedding\"], embeddingsDict[randRow[\"rKey\"]][\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000009"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same thing!\n",
    "#check passes \n",
    "randRow[\"simScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([59479338., 46855108., 36062156., 27001246., 19874420., 14838914.,\n",
       "        13012390., 14586432., 31507106., 54381700.]),\n",
       " array([0.8 , 0.82, 0.84, 0.86, 0.88, 0.9 , 0.92, 0.94, 0.96, 0.98, 1.  ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get a distribution over similarity scores \n",
    "plt.hist(pairsDf[\"simScore\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply date filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only unique pairs of articles \n",
    "pairsUnique = pairsDf.drop_duplicates(subset=[\"lKey\", \"rKey\"])\n",
    "artsDf = pd.read_csv(CLEANED_DF_PATH,usecols=[\"key\", \"date\"], sep=\"\\t\")\n",
    "\n",
    "#get a dictionary that has articles as keys and dates as values \n",
    "artsDf[\"date\"] = pd.to_datetime(artsDf[\"date\"])\n",
    "dateDict = artsDf.set_index(\"key\").to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 317598810/317598810 [1:23:32<00:00, 63363.30it/s]  \n"
     ]
    }
   ],
   "source": [
    "#get only the date indices that are a week or less apart  \n",
    "lKeys = pairsDf[\"lKey\"]\n",
    "rKeys = pairsDf[\"rKey\"]\n",
    "\n",
    "dateIndices = []\n",
    "for i in tqdm(range(len(pairsDf))): \n",
    "    lDate = dateDict[lKeys[i]][\"date\"]\n",
    "    rDate = dateDict[rKeys[i]][\"date\"]\n",
    "    \n",
    "    dateDist = (rDate - lDate).days\n",
    "    \n",
    "    #so we can have Sunday - Sunday but not Sunday - Monday \n",
    "    if np.abs(dateDist) < DATE_FILTER: \n",
    "        dateIndices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the remaining pairs \n",
    "datePairs = pairsDf.iloc[dateIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5680837721022948"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datePairs) / len(pairsDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate connected components, write to file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10:\n",
      "[237930, 38855, 6731, 6504, 5848, 4166, 3341, 3240, 3137, 3125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [05:24<10:48, 324.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10:\n",
      "[20275, 7206, 5768, 5653, 4108, 3599, 3376, 3093, 2406, 2312]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [09:57<04:54, 294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10:\n",
      "[1378, 1026, 737, 622, 525, 516, 477, 464, 442, 391]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [13:48<00:00, 276.07s/it]\n"
     ]
    }
   ],
   "source": [
    "#we want to be able to work with multiple cutoffs if need be, so output data for these cutoffs accordingly \n",
    "#file paths include the Named Entity cutoff as well as the cosine sim. cutoff \n",
    "BASE_PATH = \"/shared/3/projects/newsDiffusion/data/interim/NEREmbedding/\"\n",
    "OUT_PATHS = [\"embeddingClusterDf_2_20000_80.tsv\", \"embeddingClusterDf_2_20000_85.tsv\", \"embeddingClusterDf_2_20000_90.tsv\"]\n",
    "CUTOFFS = [.80, .85, .90]\n",
    "\n",
    "for i, cutoff in enumerate(tqdm(CUTOFFS)): \n",
    "    outPath = BASE_PATH + OUT_PATHS[i]\n",
    "    overThresh = datePairs[datePairs[\"simScore\"] >= cutoff]\n",
    "    graph = nx.from_pandas_edgelist(overThresh[[\"lKey\", \"rKey\"]], \"lKey\", \"rKey\")\n",
    "\n",
    "    components = nx.connected_components(graph)\n",
    "    compList = [comp for comp in components]\n",
    "\n",
    "    clusters = pd.DataFrame({\"cluster\":compList}) #.reset_index()\n",
    "\n",
    "    #we can remove clusters of size one \n",
    "    clusters[\"clustSize\"] = clusters[\"cluster\"].apply(lambda x: len(list(x)))\n",
    "\n",
    "    print(f'first 10:\\n{sorted(clusters[\"clustSize\"], reverse=True)[:10]}')\n",
    "    \n",
    "    clusters = clusters[clusters[\"clustSize\"] > 1]\n",
    "\n",
    "    clusters[\"clustNum\"] = list(range(0, len(clusters)))\n",
    "\n",
    "    clustDf = clusters.explode(\"cluster\").rename(columns={\"index\":\"clustNum\", \"cluster\":\"key\"})\n",
    "\n",
    "    clustSizes = pd.DataFrame(clustDf[\"clustNum\"].value_counts()).reset_index()\n",
    "\n",
    "    clustSizes.value_counts()\n",
    "    \n",
    "    #write our clusters to output file \n",
    "    clustDf.to_csv(outPath, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
