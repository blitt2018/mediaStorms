{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import evaluation\n",
    "import torch \n",
    "import torch.nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import sklearn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can't have any na rows when calculating embeddings on content \n",
    "#NOTE: when merging back in we will need to run .dropna(columns=[\"headTail\"])\n",
    "df = pd.read_csv(\"/shared/3/projects/newsDiffusion/data/interim/NEREmbedding/headTailMerged.tsv\", sep=\"\\t\", nrows = 10000, usecols=[\"key\", \"headTail\"])\n",
    "df[\"headTail\"] = df[\"headTail\"].fillna(\"\")\n",
    "#/shared/3/projects/benlitterer/localNews/mergedNewsData/mergedNER.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>headTail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>remember when : shehan ’ s polio storydan sheh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Veterans Foundation salutes Vietnam veteransTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>governor issues stay at home ordergovernor kay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>students prepare for online classesstudents ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>local banks await final rules, guidance for pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>mdc reminds people to follow health precaution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>‘ this is warfare ’ : doctor implores safe use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>las vegas home prices set record in march, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>basic money moves everyone should consider dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>uk ’ s johnson in icu overnight ; japan declar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       key                                           headTail\n",
       "0        0  remember when : shehan ’ s polio storydan sheh...\n",
       "1        1  Veterans Foundation salutes Vietnam veteransTh...\n",
       "2        2  governor issues stay at home ordergovernor kay...\n",
       "3        3  students prepare for online classesstudents ac...\n",
       "4        4  local banks await final rules, guidance for pa...\n",
       "...    ...                                                ...\n",
       "9995  9995  mdc reminds people to follow health precaution...\n",
       "9996  9996  ‘ this is warfare ’ : doctor implores safe use...\n",
       "9997  9997  las vegas home prices set record in march, but...\n",
       "9998  9998  basic money moves everyone should consider dur...\n",
       "9999  9999  uk ’ s johnson in icu overnight ; japan declar...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_DATASETS_CACHE=\"/shared/3/projects/newsDiffusion/data/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['key', 'headTail'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"/shared/3/projects/newsDiffusion/data/interim/NEREmbedding/embeddingsKeys.tsv\"\n",
    "\n",
    "#the model that performed best with the 5 random seeds \n",
    "MODEL_PATH = \"/shared/3/projects/newsDiffusion/models/2.0-biModelAblation/finalModel/135/state_dict.tar\"\n",
    "DEVICE = 0\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nACTUALLY DECIDED TO KEEP ALL DATA \\n#get only institutional national data \\ntoKeep = [\"vox\", \"cbsnews\", \"usatoday\", \"buzzfeednews\", \"businessinsider\", \"cbssports\", \"foxnews\", \"cnn\", \"cnbc\", \"huffingtonpost\", \"washingtonpost\", \"msnbc\", \"yahoonews\", \"thenewyorktimes\", \"fivethirtyeight\", \"forbes\", \"abcnews\", \"huffpost\", \"nytimes\", \"newyorkpost\", \"dailymail\", \"vanityfair\"]\\n\\n#Keep row if it is local news or if it is national news in one of the above categories \\ninstitutionalDf = df[(df[\"national\"] == False) | ((df[\"national\"] == True) & (df[\"source\"].isin(toKeep)))]\\n\\n#del df \\n\\n\\nUSED IN PREVIOUS MVP VERSION\\nget may-september 2021 data \\ninstitutionalSubset = institutional[(institutional[\"date\"] >= \"2021-05-01\") & (institutional[\"date\"] <= \"2021-09-01\")]\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ACTUALLY DECIDED TO KEEP ALL DATA \n",
    "#get only institutional national data \n",
    "toKeep = [\"vox\", \"cbsnews\", \"usatoday\", \"buzzfeednews\", \"businessinsider\", \"cbssports\", \"foxnews\", \"cnn\", \"cnbc\", \"huffingtonpost\", \"washingtonpost\", \"msnbc\", \"yahoonews\", \"thenewyorktimes\", \"fivethirtyeight\", \"forbes\", \"abcnews\", \"huffpost\", \"nytimes\", \"newyorkpost\", \"dailymail\", \"vanityfair\"]\n",
    "\n",
    "#Keep row if it is local news or if it is national news in one of the above categories \n",
    "institutionalDf = df[(df[\"national\"] == False) | ((df[\"national\"] == True) & (df[\"source\"].isin(toKeep)))]\n",
    "\n",
    "#del df \n",
    "\n",
    "\n",
    "USED IN PREVIOUS MVP VERSION\n",
    "get may-september 2021 data \n",
    "institutionalSubset = institutional[(institutional[\"date\"] >= \"2021-05-01\") & (institutional[\"date\"] <= \"2021-09-01\")]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "#sanity check \n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiModel(torch.nn.Module): \n",
    "    def __init__(self):\n",
    "        super(BiModel,self).__init__()\n",
    "        self.model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2').to(device).train()\n",
    "        self.cos = torch.nn.CosineSimilarity(dim=1, eps=1e-4)\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask): \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    #NOTE: here we expect only one batch of input ids and attention masks \n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        encoding = self.model(input_ids.squeeze(1), attention_mask=attention_mask.squeeze(1))[0]\n",
    "        meanPooled = self.mean_pooling(encoding, attention_mask.squeeze(1))\n",
    "        return meanPooled \n",
    "    \n",
    "    #NOTE: here we expect a list of two that we then unpack \n",
    "    def forward(self, input_ids, attention_mask): \n",
    "        \n",
    "        input_ids_a = input_ids[0].to(device)\n",
    "        input_ids_b = input_ids[1].to(device)\n",
    "        attention_a = attention_mask[0].to(device)\n",
    "        attention_b = attention_mask[1].to(device)\n",
    "        \n",
    "        #encode sentence and get mean pooled sentence representation \n",
    "        encoding1 = self.model(input_ids_a, attention_mask=attention_a)[0] #all token embeddings\n",
    "        encoding2 = self.model(input_ids_b, attention_mask=attention_b)[0]\n",
    "        \n",
    "        meanPooled1 = self.mean_pooling(encoding1, attention_a)\n",
    "        meanPooled2 = self.mean_pooling(encoding2, attention_b)\n",
    "        \n",
    "        pred = self.cos(meanPooled1, meanPooled2)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load trainedModel \n",
    "#device = torch.device(\"cuda:\" + str(DEVICE) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "trainedModel = BiModel()\n",
    "trainedModel.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(DEVICE))\n",
    "trainedModel = trainedModel.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e2bee4f8534546a2ebf906daebeb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"key\", \"headTail\"]])\n",
    "dataset = dataset.map(lambda x: tokenizer(x[\"headTail\"], max_length=384, padding=\"max_length\", truncation=True, return_tensors=\"pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"headTail\"])\n",
    "dataset.set_format(type=\"torch\", columns=[\"key\", \"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a1960dbff84a10b32b191107cf60ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = []\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "outFile = open(OUTPUT_PATH, \"w\")\n",
    "for i, batch in tqdm(enumerate(loader)): \n",
    "    #print(batch)\n",
    "    ids = batch[\"input_ids\"].to(device)\n",
    "    mask = batch[\"attention_mask\"].to(device)\n",
    "    keys = batch[\"key\"].tolist()\n",
    "    \n",
    "    encodingList = trainedModel.encode(ids, mask).detach().to(\"cpu\").tolist()\n",
    "    \n",
    "    for i in range(len(keys)): \n",
    "        key = keys[i]\n",
    "        encoding = encodingList[i]\n",
    "        outFile.write(str(key) + \"\\t\" + str(encoding) + \"\\n\")\n",
    "        \n",
    "outFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
