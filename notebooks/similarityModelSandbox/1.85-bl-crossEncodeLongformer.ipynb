{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Article Similarity Modelling\n",
    "- Cross encoding \n",
    "- Translated data \n",
    "- Using Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch \n",
    "import random\n",
    "from torch import nn\n",
    "from transformers import LongformerConfig, LongformerModel, PreTrainedTokenizer, LongformerTokenizer\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "import transformers\n",
    "import seaborn as sns\n",
    "import sys\n",
    "#Build up to SBERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "for actually running this code in a file \n",
    "\n",
    "jsonPath = sys.argv[1]\n",
    "with open(jsonPath, \"r\") as f: \n",
    "    my_dict = json.load(f)\n",
    "    BAT\n",
    "EPOCHS = int(my_dict[\"EPOCHS\"])\n",
    "BATCH_SIZE = int(my_dict[\"BATCH_SIZE\"])\n",
    "DROPOUT = float(my_dict[\"DROPOUT\"])\n",
    "REG_ALPHA = float(my_dict[\"REG_ALPHA\"])\n",
    "\"\"\"\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 4 \n",
    "DROPOUT = .25\n",
    "REG_ALPHA = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceNum = 5\n",
    "device = torch.device(\"cuda:\" + str(deviceNum) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.memory_free: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 0.000000GB\n"
     ]
    }
   ],
   "source": [
    "def check_mem():\n",
    "    torch.cuda.empty_cache()\n",
    "    a = torch.cuda.memory_allocated(deviceNum)/1024/1024/1024\n",
    "    r = torch.cuda.memory_reserved(deviceNum)/1024/1024/1024\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%a)\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%r)\n",
    "    print(\"torch.cuda.memory_free: %fGB\"%(r-a))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(deviceNum)/1024/1024/1024))\n",
    "check_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seeds \n",
    "torch.manual_seed(85)\n",
    "random.seed(85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/shared/3/projects/benlitterer/localNews/NetworkMVP/translatedCleaned.tsv\", sep=\"\\t\")\n",
    "#df = df.loc[(df[\"url1_lang\"] == \"en\") & (df[\"url2_lang\"] == \"en\")]\n",
    "\n",
    "groundTruths = [\"Overall\"]\n",
    "features = ['text1', 'text2', 'title1', 'title2', 'url1_lang', 'url2_lang']\n",
    "toSelect = groundTruths + features \n",
    "\n",
    "#get only the columns we need \n",
    "#TODO: do we need \"pair_id\"? \n",
    "leanDf = df[toSelect].dropna()\n",
    "\n",
    "#rescale data from (0, 4): (0, 1)\n",
    "for colName in groundTruths: \n",
    "    leanDf[colName] = 1 - ((leanDf[colName] - 1) / 3)\n",
    "\n",
    "#reset index so it is contiguous set of numbers \n",
    "leanDf = leanDf.reset_index(drop=True)\n",
    "\n",
    "#now combine title and text together \n",
    "#first add \". \" to title \n",
    "leanDf[\"title1\"] = leanDf[\"title1\"].apply(lambda x: x + \". \")\n",
    "leanDf[\"title2\"] = leanDf[\"title2\"].apply(lambda x: x + \". \")\n",
    "\n",
    "leanDf[\"text1\"] = leanDf[\"title1\"] + leanDf[\"text1\"]\n",
    "leanDf[\"text2\"] = leanDf[\"title2\"] + leanDf[\"text2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'url1_lang', 'url2_lang', 'pair_id', 'link1', 'link2',\n",
       "       'ia_link1', 'ia_link2', 'Geography', 'Entities', 'Time', 'Narrative',\n",
       "       'Overall', 'Style', 'Tone', 'id1', 'id2', 'ogText1', 'ogTitle1',\n",
       "       'ogText2', 'ogTitle2', 'text1', 'title1', 'text2', 'title2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    2296\n",
       "de     845\n",
       "es     560\n",
       "tr     455\n",
       "pl     310\n",
       "ar     268\n",
       "fr      72\n",
       "Name: url2_lang, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leanDf[\"url2_lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: do a language cutoff \n",
    "#langList = [\"en\", \"fr\", \"es\"]\n",
    "#leanDf = leanDf[(leanDf[\"url1_lang\"].isin(langList)) & (leanDf[\"url2_lang\"].isin(langList))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-f73c78befe86>:13: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  trainDf = leanDf.loc[set(leanDf.index) - set(validIndices)]\n"
     ]
    }
   ],
   "source": [
    "#we only want to sample validation data from the pairs that are both english \n",
    "enDf = leanDf[(leanDf[\"url1_lang\"] == \"en\") & (leanDf[\"url2_lang\"] == \"en\")]\n",
    "validProp = .1\n",
    "validCount = int(validProp * len(enDf))\n",
    "print(validCount)\n",
    "validIndices = random.sample(list(enDf.index), validCount)\n",
    "\n",
    "#get dataframe with indices of only the original english pairs \n",
    "validDf = enDf.loc[validIndices]\n",
    "\n",
    "#train data should be all rows that aren't in the validation set \n",
    "#here we are taking a set difference and then indexing what remains \n",
    "trainDf = leanDf.loc[set(leanDf.index) - set(validIndices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data loaded in properly \n",
    "trainDataset = Dataset.from_pandas(trainDf)\n",
    "validDataset = Dataset.from_pandas(validDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link: https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "#example of tokenizing \n",
    "#tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('Giyaseddin/distilbert-base-cased-finetuned-fake-and-real-news-dataset')\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259beba6c8904655834491b97aa6deaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4633 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da998a423a5f40f2bee5cbee6432fd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "trainDataset = trainDataset.map(lambda x: tokenizer(x[\"text1\"], x[\"text2\"], max_length=1024, padding=\"max_length\", truncation=True))\n",
    "validDataset = validDataset.map(lambda x: tokenizer(x[\"text1\"], x[\"text2\"], max_length=1024, padding=\"max_length\", truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only need the input information \n",
    "trainDataset = trainDataset.remove_columns([\"text1\", \"text2\", \"__index_level_0__\"])\n",
    "validDataset = validDataset.remove_columns([\"text1\", \"text2\", \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataset features to PyTorch tensors\n",
    "formatColumns = groundTruths + [\"input_ids\", \"attention_mask\"]\n",
    "validDataset.set_format(type='torch', columns=formatColumns)\n",
    "trainDataset.set_format(type='torch', columns=formatColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dataloader\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    trainDataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "validLoader = torch.utils.data.DataLoader(\n",
    "    validDataset, batch_size=1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096',output_hidden_states = True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "for i, batch in enumerate(validLoader): \n",
    "    if i < 3: \n",
    "        out = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"last_hidden_state\"][:,1,:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDropModel(nn.Module): \n",
    "    def __init__(self, device, DROPOUT):\n",
    "        super(RDropModel,self).__init__()\n",
    "        #test getting the longformer model going \n",
    "        self.model = LongformerModel.from_pretrained('allenai/longformer-base-4096',output_hidden_states = True)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.GELU = nn.GELU\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.l1 = nn.Linear(768, 512).to(device)\n",
    "        self.l2 = nn.Linear(512, 250).to(device)\n",
    "        self.l3 = nn.Linear(250, 1).to(device)\n",
    "        self.loss_func = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask): \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask): \n",
    "        #encode sentence and get mean pooled sentence representation\n",
    "        #when using Lonformer...\n",
    "        #index 0 gives activations from final layers \n",
    "        #index 1 gives pooled output from CLS token \n",
    "        #index 2 gives all activations from all 12 layers + 1 layer of input \n",
    "        \n",
    "        #referring here to: \n",
    "        #https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/longformer#transformers.LongformerModel.forward\n",
    "        encoding = self.model(input_ids, attention_mask=attention_mask)[1] \n",
    "        #Debugging: print(encoding.squeeze().shape)\n",
    "        \n",
    "        \n",
    "        #squeeze to remove extra dimension. Gives us 500 x 750 \n",
    "        #first one is cls token \n",
    "        #meanPooled = self.mean_pooling(encoding, attention_mask)\n",
    "        #token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        \n",
    "        \n",
    "        #NOTE: Since dropout is random we simply send data through twice \n",
    "        #to get two predictions that have some noise \n",
    "        out = self.l1(encoding)\n",
    "        out = self.ReLU(out)\n",
    "       \n",
    "        out = self.l2(out)\n",
    "        out = self.ReLU(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        pred1 = self.l3(out)\n",
    "        \n",
    "        #print(\"pred1 shape\")\n",
    "        #print(pred1.shape)\n",
    "        \n",
    "        encoding = self.model(input_ids, attention_mask=attention_mask)[1]  #all token embeddings\n",
    "        \n",
    "        #squeeze to remove extra dimension. Gives us 500 x 750\n",
    "        #first one is cls token \n",
    "        #meanPooled = self.mean_pooling(, attention_mask)\n",
    "        \n",
    "        #NOTE: Since dropout is random we simply send data through twice \n",
    "        #to get two predictions that have some noise \n",
    "        out = self.l1(encoding)\n",
    "        out = self.ReLU(out)\n",
    "        \n",
    "        out = self.l2(out)\n",
    "        out = self.ReLU(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        pred2 = self.l3(out)\n",
    "        return pred1, pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RDropModel(device, .25).to(device)\n",
    "\n",
    "#TODO: double check on if reduction=\"mean\" is the right move here...\n",
    "#could cosine similarity also work..? I think that is between the two predicted vectors though.. \n",
    "loss_func = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "trainLen = len(trainDataset)\n",
    "\n",
    "# we would initialize everything first\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
    "\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int((trainLen*EPOCHS) / BATCH_SIZE)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps - warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the loss across multiple different objectives. \n",
    "Since overall is most important it gets more weight. \n",
    "\"\"\"\n",
    "def getWeightedLoss(predTens, gtTens):\n",
    "    #try getting rid of Tone and Style \n",
    "    LOSS_WEIGHTS = [1]\n",
    "    loss = 0.0\n",
    "    for i in range(len(LOSS_WEIGHTS)): \n",
    "        \n",
    "        #get ground truth value associated with this column name \n",
    "        currGT = gtTens[:, :, i]\n",
    "        \n",
    "        #TODO: figure out how to index properly here \n",
    "        pred = predTens[:, :, i]\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"pred\")\n",
    "        print(pred)\n",
    "        print(pred.shape)\n",
    "        print(\"GT\")\n",
    "        print(currGT)\n",
    "        print(currGT.shape)\n",
    "        \"\"\"\n",
    "        #get loss \n",
    "        loss += (loss_func(pred, currGT) * LOSS_WEIGHTS[i])\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(): \n",
    "    model.eval()\n",
    "    lossList = []\n",
    "    pred = []\n",
    "    GT = []\n",
    "\n",
    "    i = True \n",
    "    for batch in validLoader: \n",
    "        \n",
    "        # prepare batches and more all to the active device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #label = batch['ground_truth'].to(device).unsqueeze(1)\n",
    "\n",
    "        #send batch info through model \n",
    "        pred1, pred2 = model(input_ids, attention_mask)\n",
    "        pred1 = pred1.unsqueeze(0)\n",
    "        pred2 = pred2.unsqueeze(0)\n",
    "        #print(pred1)\n",
    "        gts = torch.stack([batch[colName] for colName in groundTruths], 0).to(device).T.unsqueeze(0)\n",
    "        #return gts, pred1\n",
    "        \n",
    "        #get wegihted loss relating to label prediction \n",
    "        loss1 = getWeightedLoss(gts, pred1)\n",
    "        loss2 = getWeightedLoss(gts, pred2)\n",
    "        loss_b = .5*(loss1 + loss2)\n",
    "        \n",
    "        #get loss relating to invariance to dropout \n",
    "        #NOTE:\n",
    "        loss_r = getWeightedLoss(pred1, pred2)\n",
    "        \n",
    "        #combine losses with alpha hyperparam \n",
    "        loss = REG_ALPHA*loss_r + (1-REG_ALPHA)*loss_b\n",
    "        lossList.append(loss.item())\n",
    "        \n",
    "        #careful about dimensions...\n",
    "        #we will definitely have 3 dimensions here, if they sum to 3 then \n",
    "        #that means every dimension is one \n",
    "        if sum(pred1.size()) != 3: \n",
    "            pred.append([float(item) for item in list(pred1.squeeze())])\n",
    "            GT.append([float(item) for item in list(gts.squeeze())])\n",
    "        else: \n",
    "            pred.append([float(item) for item in [pred1.squeeze()]])\n",
    "            GT.append([float(item) for item in [gts.squeeze()]])\n",
    "            \n",
    "        if not (len(lossList) == len(pred) == len(pred)):\n",
    "            print(\"lens not equal\")\n",
    "    #print(vGT)\n",
    "    return [lossList, pred, GT]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720226b1dd2f4bd6bcf187057879af14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2317 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting validation\n",
      "finishing validation\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB (GPU 5; 10.92 GiB total capacity; 9.94 GiB already allocated; 15.38 MiB free; 10.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b07e84f7844f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#send batch info through model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mpred1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mpred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-5c08af4427fd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(pred1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#all token embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#squeeze to remove extra dimension. Gives us 500 x 750\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         )\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1709\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1288\u001b[0m                 )\n\u001b[1;32m   1289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1291\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m     ):\n\u001b[0;32m-> 1214\u001b[0;31m         self_attn_outputs = self.attention(\n\u001b[0m\u001b[1;32m   1215\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     ):\n\u001b[0;32m-> 1150\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m   1151\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mglobal_key_attn_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m         attn_probs = nn.functional.softmax(\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mattn_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         )  # use fp32 for numerical stability\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1844\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 5; 10.92 GiB total capacity; 9.94 GiB already allocated; 15.38 MiB free; 10.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "trainDict = {}\n",
    "lossList = []\n",
    "validMetrics = []\n",
    "trainMetrics = []\n",
    "subLossList = []\n",
    "# increase from 1 epoch if need be \n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    model.train()  # make sure model is in training mode\n",
    "    \n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    loop = tqdm(trainLoader, leave=True)\n",
    "    \n",
    "    print(\"starting validation\")\n",
    "    validMetrics.append(validation())\n",
    "    #validTester = validation()\n",
    "    print(\"finishing validation\")\n",
    "    \n",
    "   \n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(loop): \n",
    "        # zero all gradients on each new step\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # prepare batches and more all to the active device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "    \n",
    "        #send batch info through model \n",
    "        pred1, pred2 = model(input_ids, attention_mask)\n",
    "        pred1 = pred1.unsqueeze(0)\n",
    "        pred2 = pred2.unsqueeze(0)\n",
    "        \n",
    "        gts = torch.stack([batch[colName] for colName in groundTruths], 0).T.to(device).unsqueeze(0)\n",
    "        \n",
    "        \n",
    "        #get loss relating to label prediction \n",
    "        loss1 = getWeightedLoss(gts, pred1)\n",
    "        loss2 = getWeightedLoss(gts, pred2)\n",
    "        loss_b = .5*(loss1 + loss2)\n",
    "        \n",
    "        #get loss relating to invariance to dropout \n",
    "        loss_r = getWeightedLoss(pred1, pred2)\n",
    "        \n",
    "        #combine losses with alpha hyperparam \n",
    "        loss = REG_ALPHA*loss_r + (1-REG_ALPHA)*loss_b\n",
    "        \n",
    "        # using loss, calculate gradients and then optimize\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        #get mean loss over last 20 batches \n",
    "        if i % 20 == 0 and i > 0: \n",
    "            #print(subLossList)\n",
    "            lossList.append(np.mean(subLossList))\n",
    "            subLossList = []\n",
    "        \n",
    "        subLossList.append(float(loss.item()))\n",
    "        \n",
    "        # update learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # update the TDQM progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "validMetrics.append(validation())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossSmoothing = 20\n",
    "lossIndex = [(i * lossSmoothing)/len(loop) for i in range(len(lossList))]\n",
    "plt.plot(lossIndex, lossList)\n",
    "plt.xlabel(\"Batch Num\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Train Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validArr = np.array(validMetrics[4])\n",
    "#print(validArr.shape)\n",
    "validArr = np.array(validMetrics)\n",
    "#validArr = validArr[5,:,:]\n",
    "#np.corrcoef(test[1], test[2])\n",
    "\n",
    "outDfList = []\n",
    "iterList = []\n",
    "corrList = []\n",
    "#go through each validation step\n",
    "\n",
    "for i in range(validArr.shape[0]): \n",
    "    print(i)\n",
    "    subDf = pd.DataFrame(validArr[i].T)\n",
    "    subDf.columns = [\"loss\", \"pred\", \"true\"]\n",
    "\n",
    "    predCols = [\"pred\" + item for item in groundTruths]\n",
    "    gtCols = [\"gt\" + item for item in groundTruths]\n",
    "    \n",
    "    \n",
    "    subDf[predCols] = pd.DataFrame(subDf[\"pred\"].tolist(), index=subDf.index)\n",
    "    subDf[gtCols] = pd.DataFrame(subDf[\"true\"].tolist(), index=subDf.index)\n",
    "    \n",
    "    corrScores = []\n",
    "    for colName in groundTruths: \n",
    "        corr = np.corrcoef(subDf[\"pred\" + colName], subDf[\"gt\" + colName])[1, 0]\n",
    "        corrScores.append(corr)\n",
    "    corrList.append(corrScores)\n",
    "    \n",
    "corrDf = pd.DataFrame(corrList, columns=groundTruths)\n",
    "\n",
    "#plt.plot(iterList, corrList)\n",
    "#plt.xlabel(\"batch num\")\n",
    "#plt.ylabel(\"pearson correlation\")\n",
    "#plt.title(\"validation eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validArr[1:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=corrDf)\n",
    "plt.title(\"Training Over all Objectives\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, EPOCHS)\n",
    "#Grabbed this line from: https://www.statology.org/seaborn-legend-outside/\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(subDf[\"gtOverall\"], subDf[\"predOverall\"], alpha = .2)\n",
    "plt.title(\"'Overall' predicted vs. ground truth \")\n",
    "plt.xlabel(\"ground truth\")\n",
    "plt.ylabel(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(label)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainDf[\"testLoss\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
