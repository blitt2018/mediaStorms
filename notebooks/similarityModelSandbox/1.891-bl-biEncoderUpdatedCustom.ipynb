{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch \n",
    "import transformers\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import RobertaTokenizer, PreTrainedTokenizer, DistilBertTokenizer, DistilBertModel, RobertaModel\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, util\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "import random\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "from torch import nn\n",
    "#Build up to SBERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceNum = 3\n",
    "device = torch.device(\"cuda:\" + str(deviceNum) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GRAD_ACC = 6\n",
    "EPOCHS = 2\n",
    "FOLDS = 5\n",
    "SEED = 85\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "#set seeds \n",
    "torch.manual_seed(85)\n",
    "random.seed(85)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"/shared/3/projects/benlitterer/localNews/NetworkMVP/translatedCleaned.tsv\", sep=\"\\t\")\n",
    "df = pd.read_csv(\"/home/blitt/projects/localNews/data/processed/translated_300_84.tsv\", sep=\"\\t\")\n",
    "\n",
    "#put ground truth values into a list \n",
    "df[\"ground_truth\"] = df['Overall']\n",
    "\n",
    "#get only the columns we need \n",
    "#TODO: do we need \"pair_id\"? \n",
    "#leanDf = df[[\"ground_truth\",  'text1', 'text2', 'title1', 'title2', 'url1_lang', 'url2_lang']].dropna()\n",
    "#for when using merged text\n",
    "leanDf = df[[\"ground_truth\",  'text1Merged', 'text2Merged', 'url1_lang', 'url2_lang']].dropna()\n",
    "\n",
    "#rescale data from (0, 4): (0, 1)\n",
    "leanDf[\"ground_truth\"] = 1 - ((leanDf[\"ground_truth\"] - 1) / 3)\n",
    "\n",
    "#reset index so it is contiguous set of numbers \n",
    "leanDf = leanDf.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiModel(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(BiModel,self).__init__()\n",
    "        self.model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2').to(device).train()\n",
    "        self.cos = torch.nn.CosineSimilarity(dim=1, eps=1e-4)\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask): \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask): \n",
    "        \n",
    "        input_ids_a = input_ids[0].to(device)\n",
    "        input_ids_b = input_ids[1].to(device)\n",
    "        attention_a = attention_mask[0].to(device)\n",
    "        attention_b = attention_mask[1].to(device)\n",
    "        \n",
    "        #encode sentence and get mean pooled sentence representation \n",
    "        encoding1 = self.model(input_ids_a, attention_mask=attention_a)[0] #all token embeddings\n",
    "        encoding2 = self.model(input_ids_b, attention_mask=attention_b)[0]\n",
    "        \n",
    "        meanPooled1 = self.mean_pooling(encoding1, attention_a)\n",
    "        meanPooled2 = self.mean_pooling(encoding2, attention_b)\n",
    "        \n",
    "        pred = self.cos(meanPooled1, meanPooled2)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "first = [1, 2, 3]\n",
    "second = [4, 5, 6]\n",
    "first += second\n",
    "print(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateBi(model, validLoader, loss_func):\n",
    "    model.eval()\n",
    "   \n",
    "    preds = []\n",
    "    gts = []\n",
    "    \n",
    "    for batch in tqdm(validLoader): \n",
    "        \n",
    "        input_ids = [batch[\"text1Merged_input_ids\"], batch[\"text2Merged_input_ids\"]]\n",
    "        attention_masks = [batch[\"text1Merged_attention_mask\"], batch[\"text2Merged_attention_mask\"]]\n",
    "        pred = model(input_ids, attention_masks)\n",
    "        gt = batch[\"ground_truth\"].to(device)\n",
    "        \n",
    "        preds += list(pred.detach().cpu().tolist())\n",
    "        gts += list(gt.detach().cpu().tolist())\n",
    "    corr = np.corrcoef(preds, gts)[1,0]\n",
    "    print(corr)\n",
    "    model.train()\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBi(trainDataset, validDataset): \n",
    "    model = BiModel().to(device)\n",
    "    \n",
    "    # we would initialize everything first\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=2e-6)\n",
    "    \n",
    "    # and setup a warmup for the first ~10% steps\n",
    "    total_steps = int(len(trainDataset) / BATCH_SIZE)*EPOCHS\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    #TODO: change warmup steps back after \n",
    "    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps - warmup_steps)\n",
    "\n",
    "    loss_func = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validLoader = torch.utils.data.DataLoader(validDataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"EPOCH: \" + str(epoch))\n",
    "        validateBi(model, validLoader, loss_func)\n",
    "        \n",
    "        model.train()  # make sure model is in training mode\n",
    "\n",
    "        #DEBUGGING\n",
    "        #prevParam = list(model.parameters())[0].clone()\n",
    "        for batch in tqdm(trainLoader):\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            input_ids = [batch[\"text1Merged_input_ids\"], batch[\"text2Merged_input_ids\"]]\n",
    "            attention_masks = [batch[\"text1Merged_attention_mask\"], batch[\"text2Merged_attention_mask\"]]\n",
    "            pred = model(input_ids, attention_masks)\n",
    "            \n",
    "            gt = batch[\"ground_truth\"].to(device)\n",
    "            loss = loss_func(pred, gt)\n",
    "            \n",
    "            # using loss, calculate gradients and then optimize\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "            #DEBUGGING\n",
    "            \"\"\"\n",
    "            print(loss)\n",
    "            print(scheduler.get_last_lr())\n",
    "            \n",
    "            #see if model params have changed\n",
    "            param = list(model.parameters())[0].clone()\n",
    "            print(torch.equal(param.data, prevParam.data))\n",
    "            \"\"\"\n",
    "    print(\"final validation\")\n",
    "    validateBi(model, validLoader, loss_func)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total df len: 4806\n",
      "English df len: 1738\n",
      "###### 0 ######\n",
      "Train df len: 4458\n",
      "Valid df len: 348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fde1e6a8894608a715a5279cfd9aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4458 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f326cb84de43df85eb66e51df4b278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bfcdc5451b48fda21b3e7d98de3662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4458 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336eeba435144dc8934be3da4b811935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d72599506004b53bb1d5cb0752dfa14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8602128160869816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1d5edd605848b2b6b2fdb6fa20e140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bf4cfe7eb94d11bbfa6add93340985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8948054593394716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d834adcce6440c4a33421495c4d63c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e8f1dd54b247d994de56b08316f257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8963764537915694\n",
      "###### 1 ######\n",
      "Train df len: 4458\n",
      "Valid df len: 348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c297d33cd4c40bd8178b62cbbd3a269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4458 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55af45305d62439686dfd50f59c163d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44090fe72f12484f86502d0ea0b6f431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4458 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af00ff9417054825931ed2a01367057e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5af6cfd58a4d47a5d37741077642dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8653715387881095\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2fd43094614d5491e087d60dd70f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe175f46768406f97fe89bd276dd4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9070863290135546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8332a73461dd4c8e90a667b039e821ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4e1cb0f6e146a38bfdbdd85276ce57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908516931389223\n",
      "###### 2 ######\n",
      "Train df len: 4458\n",
      "Valid df len: 348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4502359b2b634580840ee9345cc9fafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4458 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764c8aca3a724920b087cc9f56321762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a652d895cb7040a5a846efdf2911524c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4458 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbcdb58902340e38ed1ce81c5c5ef5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a17d86728c446db6a8c103b079fd3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8306072507600665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0647f69fa2ef476faa5e3e817e5e17e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf72c4e29de46e78d1fd8c98d3c6d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8628814106468531\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3afa05cc064c7c93f91b8492642588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7d5ecec435455093b975c5f7c44ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8630012597730979\n",
      "###### 3 ######\n",
      "Train df len: 4459\n",
      "Valid df len: 347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed4e548068b446baca912fc3df2d158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4459 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2403be9b120447fe9351a7cedd306033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fceb96045fca459d9a27fc7f89aa6980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4459 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d13306651004a96bfba45cdcbae79c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6331bdb658ef4d0aa7404aa3a9e6d9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8726901595154178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93c4887dda841f5ae191ae98545c8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5593a86acd68472bb244a4ff1590483a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8893438644689854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e2486bf1e1431cbd959ebcf5501f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e387aad5ea4e9f8852d0f918ff0304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8898265273257309\n",
      "###### 4 ######\n",
      "Train df len: 4459\n",
      "Valid df len: 347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08087c00220e4954833a3bf120e90ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4459 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b8d5a5e4644601a698f57eba51c74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5540ccb0c6b540a580af238f85d89885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4459 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a7777d7c904609968cc439ac504357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d17b5d6fc574d72bbcaec65d02f0b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867529841724465\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab7964181b747608e4f65d343004300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48faac0559b6485c88824a433dd32fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8851164449352588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a30335223c4cf995fb245b81f2e0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d791b98c4ab54b3b800b5dc4ea765cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8863489719478314\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "transformers.logging.set_verbosity_error()\n",
    "biTokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "crossTokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#NOTE: THIS LINE IS ONLY FOR QUICK TRAINING CHECK \n",
    "#leanDf = leanDf.iloc[:100, :]\n",
    "\n",
    "#we only want to sample validation data from the pairs that are both english \n",
    "enDf = leanDf[(leanDf[\"url1_lang\"] == \"en\") & (leanDf[\"url2_lang\"] == \"en\")]\n",
    "\n",
    "print(\"Total df len: \" +  str(len(leanDf)))\n",
    "print(\"English df len: \" +  str(len(enDf)))\n",
    "\n",
    "biCorrs = []\n",
    "#we create splits based on the position (not the actual index) of rows in enDf\n",
    "#the idea is to get a split of the english dataset to set aside and then \n",
    "#grab everything else in the en + translated dataset to train on \n",
    "for i, (train_index, valid_index) in enumerate(kf.split(enDf)): \n",
    "    \n",
    "    #grab the rows in enDf corresponding to the positions of our split \n",
    "    validDf = enDf.iloc[valid_index]\n",
    "    \n",
    "    #now get the actual indicies that have been selected\n",
    "    #and subtract the indices in trainDf away from those \n",
    "    remainingIndices = list(set(leanDf.index) - set(validDf.index))\n",
    "    trainDf = leanDf.loc[remainingIndices]\n",
    "    validDf = validDf.reset_index(drop=True)\n",
    "    \n",
    "    print(\"###### \" + str(i).upper() + \" ######\")\n",
    "    print(\"Train df len: \" + str(len(trainDf)))\n",
    "    print(\"Valid df len: \" + str(len(validDf)))\n",
    "    \n",
    "    \n",
    "    trainDataset = Dataset.from_pandas(trainDf)\n",
    "    validDataset = Dataset.from_pandas(validDf)\n",
    "    \n",
    "    all_cols = [\"ground_truth\"]\n",
    "    #NOTE: here we use the merged text\n",
    "    for part in [\"text1Merged\", \"text2Merged\"]: \n",
    "        #tokenizes each row of the dataset and gives us back tuple of lists \n",
    "        trainDataset = trainDataset.map(lambda x: biTokenizer(x[part], max_length=384, padding=\"max_length\", truncation=True))\n",
    "        validDataset = validDataset.map(lambda x: biTokenizer(x[part], max_length=384, padding=\"max_length\", truncation=True))\n",
    "        \n",
    "        for col in ['input_ids', 'attention_mask']: \n",
    "            trainDataset = trainDataset.rename_column(col, part+'_'+col)\n",
    "            validDataset = validDataset.rename_column(col, part+'_'+col)\n",
    "            all_cols.append(part+'_'+col)\n",
    "            \n",
    "    trainDataset.set_format(type='torch', columns=all_cols)\n",
    "    validDataset.set_format(type='torch', columns=all_cols)\n",
    "    \n",
    "    biCorrs.append(trainBi(trainDataset, validDataset))\n",
    "\n",
    "    \"\"\"\n",
    "    #TRAIN CROSS ENCODER\n",
    "    #get data loaded in properly \n",
    "    trainDataset = Dataset.from_pandas(trainDf)\n",
    "    validDataset = Dataset.from_pandas(validDf)\n",
    "    \n",
    "    #NOTE: here we use the merged text\n",
    "    trainDataset = trainDataset.map(lambda x: tokenizer(x[\"text1Merged\"], x[\"text2Merged\"], max_length=512, padding=\"max_length\", truncation=True))\n",
    "    validDataset = validDataset.map(lambda x: tokenizer(x[\"text1Merged\"], x[\"text2Merged\"], max_length=512, padding=\"max_length\", truncation=True))\n",
    "    \n",
    "\n",
    "    #only need the input information \n",
    "    trainDataset = trainDataset.remove_columns([\"text1Merged\", \"text2Merged\", \"__index_level_0__\"])\n",
    "    validDataset = validDataset.remove_columns([\"text1Merged\", \"text2Merged\", \"__index_level_0__\"])\n",
    "\n",
    "    # convert dataset features to PyTorch tensors\n",
    "    validDataset.set_format(type='torch', columns=[\"ground_truth\", \"input_ids\", \"attention_mask\"])\n",
    "    trainDataset.set_format(type='torch', columns=[\"ground_truth\", \"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    \n",
    "    validMetrics = train(trainDataset, validDataset)\n",
    "    metrics.append(validMetrics)\n",
    "    \n",
    "    del trainDataset\n",
    "    del validDataset\n",
    "    \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = BiModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.2136e-02, -4.1067e-02,  2.0559e-02, -2.5608e-02, -8.1214e-02,\n",
       "        -5.0451e-02, -9.2575e-02,  5.8284e-02, -1.0750e-02,  2.0278e-01,\n",
       "         1.2713e-02, -6.3587e-02,  6.8787e-02, -1.0403e-02, -1.4380e-02,\n",
       "         7.3561e-02, -3.4523e-02,  1.1633e-01,  1.6805e-02, -2.1002e-02,\n",
       "         5.0159e-02,  1.4371e-01, -4.8371e-02,  5.3918e-03,  3.3420e-02,\n",
       "         6.3143e-02, -2.6937e-02, -1.2208e-02, -4.9116e-02, -3.3328e-02,\n",
       "        -3.0575e-02,  4.8712e-02,  1.5139e-02,  4.6669e-02, -2.0943e+00,\n",
       "         7.2996e-02, -6.4886e-02,  3.7177e-02, -3.1688e-02,  1.4390e-02,\n",
       "        -4.2109e-02, -3.4859e-02, -2.5870e-02,  9.5110e-03, -2.1825e-02,\n",
       "         6.0410e-02,  8.7895e-02,  6.5424e-03,  1.5588e-02, -1.8657e-01,\n",
       "         5.4031e-02, -3.9251e-02,  3.2589e-02, -3.8241e-02, -4.9235e-02,\n",
       "        -1.7446e-02, -1.2888e-03, -5.9867e-03,  8.3567e-03, -3.7311e-02,\n",
       "        -7.0938e-02,  9.2002e-02, -1.4749e-02,  9.8001e-02,  6.8946e-02,\n",
       "         1.1815e-02, -7.0052e-03, -1.1217e-01,  4.4831e-02,  3.0972e-02,\n",
       "        -5.0502e-02, -4.4378e-02, -1.1403e-02, -3.1872e-02,  1.1436e-02,\n",
       "         1.5433e-02,  5.0693e-02, -3.4556e-02, -3.6714e-02,  3.8608e-02,\n",
       "        -5.0368e-02,  8.1040e-02,  1.1633e-01, -7.6207e-02,  2.3463e-02,\n",
       "        -1.2627e-02, -1.0386e-01,  1.1421e-01,  7.4407e-02,  7.1550e-02,\n",
       "        -4.6074e-02,  4.6291e-02, -9.8696e-02,  1.1855e-02,  9.8974e-02,\n",
       "        -4.0527e-02, -9.5348e-02, -2.1856e-02, -8.8219e-02, -2.9298e-02,\n",
       "        -1.1737e-02,  2.1279e-02,  4.7875e-02,  4.2771e-02,  1.1110e-02,\n",
       "        -1.9406e-02,  1.2664e-02,  3.9567e-03, -1.8892e-03,  1.3929e-02,\n",
       "         4.3085e-04, -5.6507e-02, -1.6306e-02,  1.9356e-02, -8.9075e-03,\n",
       "        -2.8927e-02, -5.7208e-02, -3.8315e-02, -6.1745e-02, -2.1830e-02,\n",
       "         2.9869e-02, -2.0655e-02,  7.7263e-02, -2.8699e-02, -1.6612e-02,\n",
       "        -4.0369e-02,  7.5063e-02,  1.3001e-02, -1.1503e-03, -2.4492e-02,\n",
       "        -7.4003e-02, -4.3824e-02,  1.0241e-02, -1.2675e-02, -2.1946e-01,\n",
       "         3.6192e-02,  3.0915e-02,  1.4660e-02,  6.2969e-02, -5.0397e-02,\n",
       "         1.7546e-05, -2.3251e-01, -2.0223e-02, -2.0068e-02,  2.1857e-02,\n",
       "        -3.7619e-02,  3.5118e-02,  7.1419e-02,  2.2001e-03, -5.4628e-04,\n",
       "        -3.4389e-02, -1.2025e-01, -2.3350e-02, -8.6007e-02, -8.6761e-02,\n",
       "        -4.5256e-02,  2.0432e-02,  4.7983e-03, -1.6938e-02, -2.2694e-02,\n",
       "         5.7761e-04,  3.1024e-02,  5.8374e-02,  1.2032e-04,  3.1253e-02,\n",
       "        -1.2191e-01, -2.2354e-02, -5.9440e-02, -4.1424e-02, -3.7745e-02,\n",
       "         3.5193e-02, -2.8770e-02, -4.4865e-02, -1.0261e-02, -1.3586e-02,\n",
       "        -4.1391e-02, -2.2650e-02, -6.4939e-02, -8.2277e-02, -7.7424e-03,\n",
       "        -2.4182e-02, -7.0423e-02,  2.3475e-02, -8.0556e-02,  2.1387e-02,\n",
       "        -2.1595e-02,  3.6871e-02,  3.2417e-02, -2.9493e-02, -2.5155e-01,\n",
       "        -6.5639e-02, -4.4803e-02,  5.2104e-03,  1.0102e-01,  6.3348e-02,\n",
       "        -6.9470e-02, -2.1385e-02, -1.1755e-02,  1.0254e-01, -3.4138e-02,\n",
       "         3.2865e-02, -7.3462e-02,  2.2441e-02, -2.7617e-02, -1.4721e-02,\n",
       "        -7.6561e-04,  9.6314e-02, -9.2097e-02,  3.3198e-02,  8.6215e-02,\n",
       "         2.6898e-02,  2.2670e-02, -8.7823e-02, -2.9994e-02, -2.3343e-02,\n",
       "         3.0506e-02, -2.7238e-02,  9.4848e-02,  4.2977e-02, -9.0036e-02,\n",
       "        -1.1062e-02, -1.5514e-02, -7.0008e-05, -2.2587e-02, -2.7314e-02,\n",
       "         3.9728e-02,  8.7148e-02,  1.3658e-02, -4.0689e-02,  4.1872e-02,\n",
       "        -2.7443e-01,  1.0998e-02, -7.4656e-02,  1.0541e-01, -3.7552e-02,\n",
       "         7.3852e-03, -6.5320e-02,  5.7341e-02, -1.8451e-02,  3.6374e-03,\n",
       "         5.4914e-02, -6.2688e-02,  9.2885e-02,  2.6122e-02, -5.7200e-02,\n",
       "         4.8241e-02,  3.4965e-02, -5.6238e-02,  3.1423e-02,  1.3219e-02,\n",
       "        -4.0648e-02, -1.0890e-01,  1.4192e-02, -6.7561e-02,  7.3657e-02,\n",
       "        -8.1873e-02, -1.2807e-01,  1.5693e-02, -2.2810e-02, -2.9024e-02,\n",
       "         1.3725e+00, -1.2319e-02, -9.8776e-02, -9.8791e-02,  9.2658e-04,\n",
       "        -9.2236e-03,  1.1676e-02, -3.2362e-02,  7.5798e-03,  3.2161e-02,\n",
       "        -2.4735e-02, -9.9230e-02, -8.9125e-02, -1.3985e-02,  5.0730e-02,\n",
       "        -1.7862e-01,  6.5904e-03,  1.1314e-01, -6.7594e-02,  1.7222e-02,\n",
       "        -9.4742e-03,  6.8020e-02,  9.2641e-02,  8.5200e-03,  2.1637e-02,\n",
       "        -2.7387e-03,  4.9391e-02, -1.0988e-02,  1.1795e-02, -2.3979e-02,\n",
       "        -2.3474e-02,  3.9580e-02, -3.1917e-01,  2.8400e-05,  9.1726e-03,\n",
       "         3.9785e-02,  6.7269e-03,  4.8300e-02,  2.9113e-02,  3.1331e-02,\n",
       "        -4.8720e-03,  2.4174e-02,  2.1291e-02, -7.8422e-02,  7.8556e-02,\n",
       "        -3.4075e-02, -6.7136e-02,  4.0296e-02,  1.4425e-01, -7.7924e-02,\n",
       "        -4.8519e-02, -3.6273e-02, -2.6479e-02, -3.0094e-02,  1.5213e-02,\n",
       "        -3.1185e-02, -4.1553e-02,  4.5157e-03, -1.2263e-02, -4.9573e-02,\n",
       "        -6.6792e-02,  5.8937e-02,  2.0049e-02, -4.5702e-02,  3.0607e-02,\n",
       "         8.9129e-03,  3.4547e-02,  5.8900e-03, -4.2177e-02, -2.4783e-02,\n",
       "        -2.7286e-02,  1.6304e-02, -1.2393e-02,  1.6517e-02,  3.1755e-03,\n",
       "        -8.7963e-02, -3.0646e-03,  1.8840e-02, -3.0982e-02,  1.6435e-01,\n",
       "         4.9774e-02,  7.8127e-03, -1.7493e-04,  1.2720e-02,  4.9529e-02,\n",
       "        -1.3172e-01, -7.4949e-03,  9.7216e-02, -3.0059e-02, -2.1335e-01,\n",
       "        -1.1072e-01, -1.3176e-02, -1.6731e-02, -3.7925e-02,  1.6846e-02,\n",
       "        -1.0750e-02,  7.1603e-02, -5.1401e-03, -3.5083e-02, -2.8855e-02,\n",
       "        -1.6800e-02,  3.8349e-03, -3.3621e-02, -9.1937e-04, -4.7774e-02,\n",
       "        -1.3626e-02,  3.1367e-02, -1.3194e-01, -2.6019e-02, -3.2185e-03,\n",
       "        -8.3615e-02,  8.4520e-02, -1.4738e-02,  1.5540e-01, -2.4861e-02,\n",
       "         4.4098e-02, -3.9990e-02, -1.3669e-02, -5.2955e-02,  1.4575e-02,\n",
       "         2.8408e-02, -8.3105e-03, -2.3341e-02, -6.1495e-02,  5.6224e-02,\n",
       "        -1.9319e-02,  2.8823e-02,  3.8452e-02, -3.1304e-02,  3.2802e-02,\n",
       "        -8.4896e-02, -4.8731e-02, -2.3070e-03,  1.3247e-01, -2.0172e-02,\n",
       "         2.9006e-02,  4.1780e-02, -3.4561e-02, -1.7005e-02, -1.0604e+00,\n",
       "         1.3558e-01, -3.4724e-02,  4.8020e-02, -8.9501e-02,  2.6263e-02,\n",
       "         1.4741e-02, -1.9601e-02,  1.3875e-02,  2.7372e-02,  8.0363e-03,\n",
       "        -2.0247e-03,  3.4005e-02, -3.7801e-03,  4.2801e-02, -2.3848e-02,\n",
       "         3.6934e-02,  3.2337e-02,  3.4555e-02, -2.1619e-04,  1.4330e-02,\n",
       "        -3.8477e-02, -1.0174e-02,  1.8937e-02,  3.8870e-02, -4.6058e-02,\n",
       "         4.5904e-02, -1.1731e-02, -4.0138e-02,  9.6597e-02, -1.5052e-01,\n",
       "        -1.2977e-01, -3.7090e-03, -8.6980e-01,  3.7974e-02, -3.3200e-02,\n",
       "         2.5063e-02, -5.2231e-02, -6.8510e-02, -1.4804e-03,  1.1083e-02,\n",
       "         4.3355e-02, -9.3477e-02,  3.8498e-01,  6.1599e-02,  8.3810e-02,\n",
       "        -2.2375e-02,  1.5783e-01,  4.8851e-04, -2.4957e-02,  3.7533e-02,\n",
       "         2.3156e-03, -3.2980e-02, -1.3807e-02,  6.8990e-02,  5.4624e-02,\n",
       "        -4.1164e-02, -2.5123e-02,  3.5705e-02, -3.7989e-02,  2.0049e-02,\n",
       "         1.2981e-02,  9.3566e-01,  1.6802e-02,  1.1454e-02,  3.0424e-02,\n",
       "        -2.8667e-02, -1.8284e-02,  1.1502e-02,  3.8502e-02,  8.8478e-02,\n",
       "         1.0332e-02, -1.2789e-01, -6.0658e-02, -9.7187e-02, -7.8119e-02,\n",
       "         6.6624e-03,  1.2878e-02,  2.1097e-02,  4.7925e-02, -1.2370e-02,\n",
       "         7.5927e-03,  6.6251e-02, -2.4408e-01, -8.0579e-03, -8.1955e-02,\n",
       "        -2.2224e-02,  1.1590e-01, -7.7108e-02, -1.9690e-01,  3.6873e-03,\n",
       "        -1.4956e-01, -8.4278e-03,  4.1049e-02,  3.0540e-04,  3.7404e-02,\n",
       "         3.3259e-02, -3.6986e-02, -3.6598e-02, -2.1840e-03,  3.9020e-02,\n",
       "         2.7628e-02, -6.3115e-02,  8.6935e-02,  5.3807e-02, -4.2632e-02,\n",
       "        -7.7554e-02,  1.5091e-02,  5.6076e-02, -1.6901e-02,  3.5518e-02,\n",
       "         5.5448e-02, -7.8835e-02, -4.1132e-02, -7.3689e-02, -2.5034e-02,\n",
       "        -5.6710e-02,  3.1082e-02,  2.4062e-02,  1.1814e-01, -1.9150e-02,\n",
       "        -1.9765e-01, -2.8071e-02,  9.6329e-02, -3.3405e-02,  2.0591e-03,\n",
       "        -5.8589e-03,  1.0615e-01, -1.5392e-01,  9.9910e-03, -3.0494e-02,\n",
       "         2.4068e-02, -1.1409e-01, -3.1618e-01, -5.0945e-02,  2.9576e-02,\n",
       "         3.1078e-02,  1.6876e-02, -4.1143e-02,  1.5767e-02,  4.1382e-02,\n",
       "         4.4055e-02, -5.2131e-02, -2.4622e-02, -4.7802e-03,  5.8029e-02,\n",
       "         1.8335e-02, -4.2788e-02,  5.4052e-02, -3.6439e-02,  9.2569e-02,\n",
       "        -7.5453e-02,  3.1608e-02, -3.2672e-02,  2.2288e-02,  1.9140e-02,\n",
       "        -1.7117e+00,  3.3586e-02,  6.8466e-02,  5.9038e-02,  3.8751e-02,\n",
       "         1.4121e-02,  8.9526e-02, -8.3040e-02, -1.6492e-01,  4.7561e-01,\n",
       "        -3.7859e-02, -3.0021e-01,  8.3050e-02,  3.9059e-02,  5.4825e-02,\n",
       "         1.1720e-02,  3.9102e-02,  3.0871e-02,  1.0539e+00,  1.2670e-02,\n",
       "         3.0296e-03,  7.4464e-02,  7.1911e-02,  5.3562e-02, -4.2958e-02,\n",
       "         7.6302e-03,  4.9444e-02, -4.3722e-02,  2.0762e-02,  5.6124e-02,\n",
       "         5.1389e-02,  6.5523e-02,  5.9435e-02,  1.1268e-01,  3.3145e-02,\n",
       "         2.2270e-02,  2.2241e-02, -3.6197e-03,  4.6661e-02,  2.0201e-02,\n",
       "        -4.2835e-02, -1.5088e-02,  8.1840e-02,  2.9885e-02,  1.1520e-02,\n",
       "         2.7283e-01,  6.2119e-03, -3.4734e-02, -4.2250e-02, -1.0618e-02,\n",
       "        -2.9541e-02,  5.2967e-02, -2.2434e-02, -4.7915e-02,  4.7310e-02,\n",
       "         4.8468e-02, -1.0528e-01,  5.6465e-02, -1.0831e-01, -2.8197e-02,\n",
       "        -7.9759e-02, -2.6894e-02,  1.5470e-01, -1.6311e-02, -5.2399e-03,\n",
       "        -2.7928e-02,  3.3100e-02, -1.8675e-02,  1.2185e-02, -6.5113e-03,\n",
       "        -1.7929e-02,  6.9779e-03,  2.3009e-02,  5.2191e-02, -1.8381e-02,\n",
       "         4.2997e-02,  1.6766e-02,  8.2802e-01,  1.0529e-01, -3.4717e-02,\n",
       "         3.2650e-02, -2.4480e-02, -7.3947e-03,  5.6173e-03, -2.2136e-02,\n",
       "        -1.6728e-02,  3.3999e-02, -1.4315e-02, -6.3275e-02, -1.2343e-03,\n",
       "        -8.2279e-02, -1.2350e-02,  2.9627e-02, -6.1593e-02, -3.6729e-02,\n",
       "         2.7588e-02, -2.6253e-02,  9.4621e-03, -5.5762e-02,  6.1624e-02,\n",
       "        -2.8262e-02,  5.1705e-02, -6.6576e-02,  3.4333e-02,  5.1224e-02,\n",
       "        -2.7998e-02,  2.9625e-02, -2.9300e-02,  3.6616e-02,  1.5545e-02,\n",
       "        -5.5071e-02,  8.7411e-02, -2.8055e-02,  2.0121e-02, -4.0621e-02,\n",
       "        -3.6199e-04,  7.7530e-02,  4.0332e-02, -2.1365e-02, -2.3018e-03,\n",
       "        -1.3059e-02, -1.5483e-02,  3.6794e-03, -2.2672e-02,  3.7585e-02,\n",
       "         3.2141e-02, -2.2008e-01, -1.2823e-02, -2.1084e-02,  7.2281e-02,\n",
       "         4.9765e-03,  9.2023e-02,  5.1452e-02, -2.2691e+00, -7.0630e-02,\n",
       "         7.2990e-03, -7.4471e-02,  1.0796e-02,  7.2348e-03, -1.5219e-02,\n",
       "         2.8031e-02,  2.9707e-02,  5.9122e-02,  1.0968e-02, -6.0335e-03,\n",
       "         4.2418e-02, -5.3896e-02, -6.9768e-02,  1.3778e-02,  2.4874e-02,\n",
       "        -5.0585e-02, -2.6592e-02, -4.6700e-02,  2.1999e-02,  4.6296e-02,\n",
       "         1.0076e-01, -2.1782e-02,  4.2689e-02, -5.2754e-03, -5.6779e-03,\n",
       "        -1.3581e-02, -6.0034e-02, -1.3313e-01, -1.2599e-02, -8.8780e-02,\n",
       "         2.6804e-02,  2.0586e-02, -2.9674e-02, -4.2065e-02, -4.2133e-02,\n",
       "        -2.5979e-02, -6.9031e-02,  8.3435e-03, -4.5186e-02,  2.1848e-02,\n",
       "        -4.2402e-02, -2.1006e-02, -6.4614e-03, -1.6003e-02, -5.1451e-02,\n",
       "        -9.6948e-02,  6.0879e-02,  2.5038e-02, -2.4785e-02,  1.0835e-01,\n",
       "         5.5846e-02, -3.5308e-02,  3.9076e-02,  4.7241e-03,  1.3892e-02,\n",
       "        -8.3629e-03, -8.8617e-02,  4.4807e-02, -4.3043e-02,  1.0592e-03,\n",
       "        -4.8148e-02, -3.2191e-02, -8.1429e-04,  3.9367e-02, -6.7604e-02,\n",
       "         2.7702e-02,  6.1644e-02, -6.2558e-02, -1.0537e-01,  7.6659e-02,\n",
       "         6.3739e-02,  1.8566e-02,  1.0474e-01,  4.8414e-02,  1.8754e-03,\n",
       "        -3.3814e-03, -1.1083e-01, -3.4964e-02], device='cuda:3',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param for param in model.parameters()][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c54f8c53abf4106bd21df7e2a6db71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709c5476bf8b437a8b5460f4abdf1763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/347 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ground_truth': 0.11111111100000004,\n",
       " 'text1Merged': 'Guyana: Three injured after car crashes into utility poleShare This On:  Pin 11 Shares  (NEWS ROOM GUYANA) — Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Year’s morning.  According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers.  The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet away before crashing into the utility pole and coming to a halt on a resident’s bridge.  The two occupants, 32-year-old Keron Phillips and 45-year-old Ramnand Kishwar were removed from the wreck in semi-conscious states and rushed to the Skeldon hospital.  The driver fled the scene and was subsequently apprehended at his Lot 80 Grant 1718 Crabwood Creek home in a traumatic state. He was also taken to the Skeldon Hospital where he is admitted in a stable condition.  The News Room understands that the vehicle is owned by an elderly woman, and Ramdatt took it without her knowledge.  Police Commissioner Leslie James on Wednesday disclosed that there has been an 8% increase in road fatalities in 2018.  ( 0 ) ( 0 ) Grant 1718 Crabwood Creek home in a traumatic state. He was also taken to the Skeldon Hospital where he is admitted in a stable condition.  The News Room understands that the vehicle is owned by an elderly woman, and Ramdatt took it without her knowledge.  Police Commissioner Leslie James on Wednesday disclosed that there has been an 8% increase in road fatalities in 2018.  ( 0 ) ( 0 )',\n",
       " 'text2Merged': \"Fire kills more than 30 animals at zoo in western GermanyBERLIN - A fire at a zoo in western Germany in the first minutes of 2020 killed more than 30 animals, including apes, monkeys, bats and birds, authorities said. Police said the fire may have been caused by sky lanterns launched to celebrate the new year.  Several witnesses reported that they had seen the cylindrical paper lanterns with little fires inside flying in the night sky shortly after midnight Wednesday near the Krefeld zoo, Gerd Hoppmann, the city’s head of criminal police told reporters.  “People reported seeing those sky lanterns flying at low altitude near the zoo and then it started burning,” Hoppmann said.  Police and firefighters received the first emergency calls at 12:38 a.m.  The zoo near the Dutch border said that the entire ape house burned down and more than 30 animals, including five orangutans, two gorillas, a chimpanzee and several monkeys, as well as fruit bats and birds, were killed.  Only two chimpanzees could be rescued from the flames by firefighters. They suffered burns but are in stable condition, zoo director Wolfgang Dressen said.  “It’s close to a miracle that Bally, a 40-year-old female chimpanzee, and Limbo, a younger male, survived this inferno,” Dressen said, adding that many animal handlers were in shock at the devastation.  Get more of the Star in your inbox Never miss the latest news from the Star. Sign up for our newsletters to get today's top stories, your favourite columnists and lots more in your inbox Sign Up Now  Hoppmann said some of the lanterns had handwritten notes on them.  The Krefeld zoo was opened in 1975 and attracts some 400,000 visitors each year. It will remain closed on Wednesday.\",\n",
       " 'url1_lang': 'en',\n",
       " 'url2_lang': 'en',\n",
       " 'text1Merged_input_ids': [0,\n",
       "  18790,\n",
       "  1028,\n",
       "  2097,\n",
       "  5233,\n",
       "  2048,\n",
       "  2486,\n",
       "  19123,\n",
       "  2050,\n",
       "  9714,\n",
       "  10571,\n",
       "  8171,\n",
       "  2067,\n",
       "  2027,\n",
       "  2010,\n",
       "  1028,\n",
       "  9235,\n",
       "  2344,\n",
       "  6665,\n",
       "  1010,\n",
       "  2743,\n",
       "  2286,\n",
       "  18790,\n",
       "  1011,\n",
       "  1521,\n",
       "  2097,\n",
       "  5385,\n",
       "  2028,\n",
       "  2751,\n",
       "  24739,\n",
       "  2003,\n",
       "  1041,\n",
       "  3813,\n",
       "  4654,\n",
       "  2210,\n",
       "  2023,\n",
       "  4930,\n",
       "  2010,\n",
       "  2000,\n",
       "  18085,\n",
       "  3706,\n",
       "  3640,\n",
       "  2274,\n",
       "  2350,\n",
       "  2010,\n",
       "  2051,\n",
       "  2099,\n",
       "  1525,\n",
       "  1059,\n",
       "  2855,\n",
       "  1016,\n",
       "  2433,\n",
       "  2004,\n",
       "  2596,\n",
       "  2367,\n",
       "  1014,\n",
       "  5017,\n",
       "  10014,\n",
       "  1056,\n",
       "  10699,\n",
       "  6539,\n",
       "  2585,\n",
       "  2579,\n",
       "  5537,\n",
       "  2015,\n",
       "  2574,\n",
       "  1015,\n",
       "  2099,\n",
       "  1015,\n",
       "  2218,\n",
       "  2160,\n",
       "  6448,\n",
       "  8227,\n",
       "  2854,\n",
       "  2106,\n",
       "  2005,\n",
       "  21489,\n",
       "  2047,\n",
       "  2013,\n",
       "  17749,\n",
       "  2011,\n",
       "  1041,\n",
       "  9714,\n",
       "  6540,\n",
       "  1014,\n",
       "  22740,\n",
       "  2000,\n",
       "  4066,\n",
       "  2002,\n",
       "  2052,\n",
       "  5471,\n",
       "  1016,\n",
       "  2000,\n",
       "  2743,\n",
       "  2286,\n",
       "  19825,\n",
       "  2012,\n",
       "  2100,\n",
       "  4443,\n",
       "  2062,\n",
       "  2000,\n",
       "  2308,\n",
       "  5884,\n",
       "  3640,\n",
       "  2962,\n",
       "  1014,\n",
       "  8227,\n",
       "  2854,\n",
       "  2106,\n",
       "  2443,\n",
       "  2495,\n",
       "  2001,\n",
       "  2000,\n",
       "  4320,\n",
       "  2033,\n",
       "  2361,\n",
       "  13174,\n",
       "  2002,\n",
       "  2733,\n",
       "  6679,\n",
       "  2059,\n",
       "  3267,\n",
       "  2523,\n",
       "  2189,\n",
       "  2081,\n",
       "  12898,\n",
       "  2050,\n",
       "  2000,\n",
       "  9714,\n",
       "  6540,\n",
       "  2002,\n",
       "  2750,\n",
       "  2004,\n",
       "  1041,\n",
       "  9194,\n",
       "  2010,\n",
       "  1041,\n",
       "  6323,\n",
       "  1525,\n",
       "  1059,\n",
       "  2962,\n",
       "  1016,\n",
       "  2000,\n",
       "  2052,\n",
       "  18841,\n",
       "  1014,\n",
       "  3594,\n",
       "  1015,\n",
       "  2099,\n",
       "  1015,\n",
       "  2218,\n",
       "  17714,\n",
       "  4952,\n",
       "  8113,\n",
       "  2002,\n",
       "  3433,\n",
       "  1015,\n",
       "  2099,\n",
       "  1015,\n",
       "  2218,\n",
       "  8227,\n",
       "  7233,\n",
       "  2098,\n",
       "  11386,\n",
       "  4099,\n",
       "  9032,\n",
       "  2024,\n",
       "  3722,\n",
       "  2017,\n",
       "  2000,\n",
       "  12010,\n",
       "  2003,\n",
       "  4104,\n",
       "  1015,\n",
       "  9719,\n",
       "  2167,\n",
       "  2002,\n",
       "  6764,\n",
       "  2004,\n",
       "  2000,\n",
       "  15319,\n",
       "  14277,\n",
       "  2243,\n",
       "  2906,\n",
       "  1016,\n",
       "  2000,\n",
       "  4066,\n",
       "  6787,\n",
       "  2000,\n",
       "  3500,\n",
       "  2002,\n",
       "  2005,\n",
       "  3529,\n",
       "  10443,\n",
       "  2894,\n",
       "  22346,\n",
       "  2102,\n",
       "  2016,\n",
       "  2014,\n",
       "  2847,\n",
       "  3774,\n",
       "  3950,\n",
       "  26999,\n",
       "  18085,\n",
       "  3706,\n",
       "  3640,\n",
       "  2192,\n",
       "  2003,\n",
       "  1041,\n",
       "  19690,\n",
       "  2114,\n",
       "  1016,\n",
       "  2006,\n",
       "  2005,\n",
       "  2040,\n",
       "  2583,\n",
       "  2004,\n",
       "  2000,\n",
       "  15319,\n",
       "  14277,\n",
       "  2243,\n",
       "  2906,\n",
       "  2077,\n",
       "  2006,\n",
       "  2007,\n",
       "  4918,\n",
       "  2003,\n",
       "  1041,\n",
       "  6544,\n",
       "  4654,\n",
       "  1016,\n",
       "  2000,\n",
       "  2743,\n",
       "  2286,\n",
       "  19825,\n",
       "  2012,\n",
       "  2000,\n",
       "  4320,\n",
       "  2007,\n",
       "  3083,\n",
       "  2015,\n",
       "  2023,\n",
       "  9754,\n",
       "  2454,\n",
       "  1014,\n",
       "  2002,\n",
       "  8227,\n",
       "  2854,\n",
       "  4783,\n",
       "  2169,\n",
       "  2013,\n",
       "  2306,\n",
       "  2018,\n",
       "  3720,\n",
       "  1016,\n",
       "  2614,\n",
       "  5853,\n",
       "  8890,\n",
       "  2512,\n",
       "  2010,\n",
       "  9321,\n",
       "  21366,\n",
       "  2012,\n",
       "  2049,\n",
       "  2042,\n",
       "  2046,\n",
       "  2023,\n",
       "  1026,\n",
       "  1007,\n",
       "  3627,\n",
       "  2003,\n",
       "  2350,\n",
       "  20875,\n",
       "  2003,\n",
       "  2764,\n",
       "  1016,\n",
       "  1010,\n",
       "  1018,\n",
       "  1011,\n",
       "  1010,\n",
       "  1018,\n",
       "  1011,\n",
       "  3950,\n",
       "  26999,\n",
       "  18085,\n",
       "  3706,\n",
       "  3640,\n",
       "  2192,\n",
       "  2003,\n",
       "  1041,\n",
       "  19690,\n",
       "  2114,\n",
       "  1016,\n",
       "  2006,\n",
       "  2005,\n",
       "  2040,\n",
       "  2583,\n",
       "  2004,\n",
       "  2000,\n",
       "  15319,\n",
       "  14277,\n",
       "  2243,\n",
       "  2906,\n",
       "  2077,\n",
       "  2006,\n",
       "  2007,\n",
       "  4918,\n",
       "  2003,\n",
       "  1041,\n",
       "  6544,\n",
       "  4654,\n",
       "  1016,\n",
       "  2000,\n",
       "  2743,\n",
       "  2286,\n",
       "  19825,\n",
       "  2012,\n",
       "  2000,\n",
       "  4320,\n",
       "  2007,\n",
       "  3083,\n",
       "  2015,\n",
       "  2023,\n",
       "  9754,\n",
       "  2454,\n",
       "  1014,\n",
       "  2002,\n",
       "  8227,\n",
       "  2854,\n",
       "  4783,\n",
       "  2169,\n",
       "  2013,\n",
       "  2306,\n",
       "  2018,\n",
       "  3720,\n",
       "  1016,\n",
       "  2614,\n",
       "  5853,\n",
       "  8890,\n",
       "  2512,\n",
       "  2010,\n",
       "  9321,\n",
       "  21366,\n",
       "  2012,\n",
       "  2049,\n",
       "  2042,\n",
       "  2046,\n",
       "  2023,\n",
       "  1026,\n",
       "  1007,\n",
       "  3627,\n",
       "  2003,\n",
       "  2350,\n",
       "  20875,\n",
       "  2003,\n",
       "  2764,\n",
       "  1016,\n",
       "  1010,\n",
       "  1018,\n",
       "  1011,\n",
       "  1010,\n",
       "  1018,\n",
       "  1011,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'text1Merged_attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'text2Merged_input_ids': [0,\n",
       "  2547,\n",
       "  8567,\n",
       "  2066,\n",
       "  2088,\n",
       "  2386,\n",
       "  4180,\n",
       "  2016,\n",
       "  9205,\n",
       "  2003,\n",
       "  2534,\n",
       "  2766,\n",
       "  5681,\n",
       "  4119,\n",
       "  1015,\n",
       "  1041,\n",
       "  2547,\n",
       "  2016,\n",
       "  1041,\n",
       "  9205,\n",
       "  2003,\n",
       "  2534,\n",
       "  2766,\n",
       "  2003,\n",
       "  2000,\n",
       "  2038,\n",
       "  2785,\n",
       "  2001,\n",
       "  12613,\n",
       "  2734,\n",
       "  2066,\n",
       "  2088,\n",
       "  2386,\n",
       "  4180,\n",
       "  1014,\n",
       "  2168,\n",
       "  27758,\n",
       "  1014,\n",
       "  17063,\n",
       "  1014,\n",
       "  12240,\n",
       "  2002,\n",
       "  5059,\n",
       "  1014,\n",
       "  4618,\n",
       "  2060,\n",
       "  1016,\n",
       "  2614,\n",
       "  2060,\n",
       "  2000,\n",
       "  2547,\n",
       "  2093,\n",
       "  2035,\n",
       "  2046,\n",
       "  3307,\n",
       "  2015,\n",
       "  3716,\n",
       "  26082,\n",
       "  3394,\n",
       "  2004,\n",
       "  8443,\n",
       "  2000,\n",
       "  2051,\n",
       "  2099,\n",
       "  1016,\n",
       "  2199,\n",
       "  9394,\n",
       "  2992,\n",
       "  2012,\n",
       "  2031,\n",
       "  2022,\n",
       "  2468,\n",
       "  2000,\n",
       "  18801,\n",
       "  3263,\n",
       "  26082,\n",
       "  2011,\n",
       "  2214,\n",
       "  8773,\n",
       "  2507,\n",
       "  3913,\n",
       "  2003,\n",
       "  2000,\n",
       "  2309,\n",
       "  3716,\n",
       "  3863,\n",
       "  2048,\n",
       "  7094,\n",
       "  9321,\n",
       "  2383,\n",
       "  2000,\n",
       "  1051,\n",
       "  2894,\n",
       "  8155,\n",
       "  9205,\n",
       "  1014,\n",
       "  16220,\n",
       "  4107,\n",
       "  6158,\n",
       "  9741,\n",
       "  11643,\n",
       "  1014,\n",
       "  2000,\n",
       "  2107,\n",
       "  1525,\n",
       "  1059,\n",
       "  2136,\n",
       "  2001,\n",
       "  4739,\n",
       "  2614,\n",
       "  2413,\n",
       "  12064,\n",
       "  1016,\n",
       "  1527,\n",
       "  2115,\n",
       "  2992,\n",
       "  3777,\n",
       "  2220,\n",
       "  3716,\n",
       "  26082,\n",
       "  3913,\n",
       "  2016,\n",
       "  2663,\n",
       "  8002,\n",
       "  2383,\n",
       "  2000,\n",
       "  9205,\n",
       "  2002,\n",
       "  2063,\n",
       "  2013,\n",
       "  2322,\n",
       "  5259,\n",
       "  1014,\n",
       "  1528,\n",
       "  6158,\n",
       "  9741,\n",
       "  11643,\n",
       "  2060,\n",
       "  1016,\n",
       "  2614,\n",
       "  2002,\n",
       "  21771,\n",
       "  2367,\n",
       "  2000,\n",
       "  2038,\n",
       "  5061,\n",
       "  4459,\n",
       "  2016,\n",
       "  2264,\n",
       "  1028,\n",
       "  4233,\n",
       "  1041,\n",
       "  1016,\n",
       "  1053,\n",
       "  1016,\n",
       "  2000,\n",
       "  9205,\n",
       "  2383,\n",
       "  2000,\n",
       "  3807,\n",
       "  3679,\n",
       "  2060,\n",
       "  2012,\n",
       "  2000,\n",
       "  2976,\n",
       "  23961,\n",
       "  2164,\n",
       "  5300,\n",
       "  2095,\n",
       "  2002,\n",
       "  2066,\n",
       "  2088,\n",
       "  2386,\n",
       "  4180,\n",
       "  1014,\n",
       "  2168,\n",
       "  2278,\n",
       "  2034,\n",
       "  5658,\n",
       "  13214,\n",
       "  3623,\n",
       "  1014,\n",
       "  2052,\n",
       "  23530,\n",
       "  2019,\n",
       "  1014,\n",
       "  1041,\n",
       "  9614,\n",
       "  8741,\n",
       "  2323,\n",
       "  23944,\n",
       "  2002,\n",
       "  2199,\n",
       "  17063,\n",
       "  1014,\n",
       "  2008,\n",
       "  2096,\n",
       "  2008,\n",
       "  5913,\n",
       "  12240,\n",
       "  2002,\n",
       "  5059,\n",
       "  1014,\n",
       "  2024,\n",
       "  2734,\n",
       "  1016,\n",
       "  2073,\n",
       "  2052,\n",
       "  9614,\n",
       "  8741,\n",
       "  2323,\n",
       "  23944,\n",
       "  2019,\n",
       "  2075,\n",
       "  2026,\n",
       "  10152,\n",
       "  2017,\n",
       "  2000,\n",
       "  7315,\n",
       "  2015,\n",
       "  21771,\n",
       "  1016,\n",
       "  2031,\n",
       "  4269,\n",
       "  7645,\n",
       "  2025,\n",
       "  2028,\n",
       "  2003,\n",
       "  6544,\n",
       "  4654,\n",
       "  1014,\n",
       "  9205,\n",
       "  2476,\n",
       "  13869,\n",
       "  4381,\n",
       "  2372,\n",
       "  2060,\n",
       "  1016,\n",
       "  1527,\n",
       "  2013,\n",
       "  1525,\n",
       "  1059,\n",
       "  2489,\n",
       "  2004,\n",
       "  1041,\n",
       "  9731,\n",
       "  2012,\n",
       "  3612,\n",
       "  2104,\n",
       "  1014,\n",
       "  1041,\n",
       "  2875,\n",
       "  1015,\n",
       "  2099,\n",
       "  1015,\n",
       "  2218,\n",
       "  2935,\n",
       "  9614,\n",
       "  8741,\n",
       "  2323,\n",
       "  23944,\n",
       "  1014,\n",
       "  2002,\n",
       "  15295,\n",
       "  2084,\n",
       "  1014,\n",
       "  1041,\n",
       "  3924,\n",
       "  3291,\n",
       "  1014,\n",
       "  5179,\n",
       "  2027,\n",
       "  21852,\n",
       "  1014,\n",
       "  1528,\n",
       "  4381,\n",
       "  2372,\n",
       "  2060,\n",
       "  1014,\n",
       "  5819,\n",
       "  2012,\n",
       "  2120,\n",
       "  4115,\n",
       "  28217,\n",
       "  2019,\n",
       "  2024,\n",
       "  2003,\n",
       "  5217,\n",
       "  2016,\n",
       "  2000,\n",
       "  25598,\n",
       "  1016,\n",
       "  2135,\n",
       "  2066,\n",
       "  2001,\n",
       "  2000,\n",
       "  2736,\n",
       "  2003,\n",
       "  2119,\n",
       "  2003,\n",
       "  8762,\n",
       "  2200,\n",
       "  3339,\n",
       "  2000,\n",
       "  6749,\n",
       "  2743,\n",
       "  2017,\n",
       "  2000,\n",
       "  2736,\n",
       "  1016,\n",
       "  3700,\n",
       "  2043,\n",
       "  2009,\n",
       "  2260,\n",
       "  17182,\n",
       "  2019,\n",
       "  2004,\n",
       "  2135,\n",
       "  2655,\n",
       "  1009,\n",
       "  1059,\n",
       "  2331,\n",
       "  3445,\n",
       "  1014,\n",
       "  2119,\n",
       "  8841,\n",
       "  13321,\n",
       "  2019,\n",
       "  2002,\n",
       "  7171,\n",
       "  2066,\n",
       "  2003,\n",
       "  2119,\n",
       "  2003,\n",
       "  8762,\n",
       "  3700,\n",
       "  2043,\n",
       "  2089,\n",
       "  6158,\n",
       "  9741,\n",
       "  11643,\n",
       "  2060,\n",
       "  2074,\n",
       "  2001,\n",
       "  2000,\n",
       "  26082,\n",
       "  2022,\n",
       "  2196,\n",
       "  15777,\n",
       "  3968,\n",
       "  2010,\n",
       "  2072,\n",
       "  1016,\n",
       "  2000,\n",
       "  1051,\n",
       "  2894,\n",
       "  8155,\n",
       "  9205,\n",
       "  2005,\n",
       "  2445,\n",
       "  2003,\n",
       "  3343,\n",
       "  2002,\n",
       "  17775,\n",
       "  2074,\n",
       "  4282,\n",
       "  1014,\n",
       "  2203,\n",
       "  5735,\n",
       "  2173,\n",
       "  2099,\n",
       "  1016,\n",
       "  2013,\n",
       "  2101,\n",
       "  3965,\n",
       "  2705,\n",
       "  2010,\n",
       "  9321,\n",
       "  1016,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'text2Merged_attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validDataset = Dataset.from_pandas(validDf)\n",
    "validDataset[0]\n",
    "\n",
    "for part in [\"text1Merged\", \"text2Merged\"]: \n",
    "        #tokenizes each row of the dataset and gives us back tuple of lists \n",
    "        validDataset = validDataset.map(lambda x: biTokenizer(x[part], max_length=384, padding=\"max_length\", truncation=True))\n",
    "        \n",
    "        for col in ['input_ids', 'attention_mask']: \n",
    "            validDataset = validDataset.rename_column(col, part+'_'+col)\n",
    "            all_cols.append(part+'_'+col)\n",
    "\n",
    "validDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-13-a6c3b135667f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-a6c3b135667f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for part in [\"text1Merged\", \"text2Merged\"]:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "all_cols = [\"ground_truth\"]\n",
    "    for part in [\"text1Merged\", \"text2Merged\"]: \n",
    "        #tokenizes each row of the dataset and gives us back tuple of lists \n",
    "        validDataset = validDataset.map(lambda x: biTokenizer(x[part], max_length=384, padding=\"max_length\", truncation=True))\n",
    "        \n",
    "        for col in ['input_ids', 'attention_mask']: \n",
    "            validDataset = validDataset.rename_column(col, part+'_'+col)\n",
    "            all_cols.append(part+'_'+col)\n",
    "    validDataset.set_format(type='torch', column=all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(validDataset, batch_size=4, shuffle=True)\n",
    "validDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send corrList to pickled object for later analysis \n",
    "import pickle\n",
    "out_path = \"/home/blitt/projects/localNews/models/sentEmbeddings/2.0-biCrossModel/1.0-justCross/crossCorrList.pckl\"\n",
    "f = open(out_path, \"wb\")\n",
    "\n",
    "pickle.dump(biCorrs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(biCorrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "modelPath = output_path \n",
    "os.listdir(modelPath + \"/checkpoints\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = \"/home/blitt/projects/localNews/models/sentEmbeddings/2.0-biCrossModel/justBiExperiments\"\n",
    "checkpoint_path = \"/home/blitt/projects/localNews/models/sentEmbeddings/2.0-biCrossModel/justBiExperiments/checkpoints\"\n",
    "\n",
    "modelPath = output_path + \"/checkpoints/500\"\n",
    "model = SentenceTransformer(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(validDf[\"text1Merged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
