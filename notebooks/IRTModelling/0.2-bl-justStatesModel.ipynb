{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc74c79-db1a-4d76-86b8-3e1778067044",
   "metadata": {},
   "source": [
    "create simple model draft to get outlet embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ca03ba7-ea94-4313-9f25-96acb2b1bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torchmetrics.functional.classification import f1_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "025bf065-d71e-4923-88b5-b4701e4a3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_NAME=\"testJustStates\" \n",
    "TRAIN_BATCH_SIZE=100\n",
    "VALID_BATCH_SIZE=200\n",
    "EPOCHS=1\n",
    "LR = .001\n",
    "K_FOLDS = 3\n",
    "\n",
    "#number of times we want validation to run \n",
    "VALID_COUNT = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ccbd25c-87bb-4dd7-bf77-4a3f245e2199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-e2098ce75a41>:2: DtypeWarning: Columns (2,3,10,11,12,13,14,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/shared/3/projects/newsDiffusion/data/processed/fullDataWithClustNums.tsv\", sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "# load in the news data of interest \n",
    "df = pd.read_csv(\"/shared/3/projects/newsDiffusion/data/processed/fullDataWithClustNums.tsv\", sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71fd96fa-b4e8-469a-b0f2-b0878f6a0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_PATH = \"/shared/3/projects/newsDiffusion/data/processed/IRTmodel/storyEmbeddingsMean.pkl\"\n",
    "storyEmbeddings = pd.read_pickle(PICKLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d888dc7e-0ae0-415a-ac2f-5def0566cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we give a story cluster number and get back the average embedding for that story cluster \n",
    "storyDict = storyEmbeddings.set_index(\"clustNum\")[[\"storyMean\"]].to_dict()[\"storyMean\"] \n",
    "\n",
    "#we want to get a list of all possible story clusters that an outlet can cover\n",
    "allClusts = storyEmbeddings[\"clustNum\"].tolist()\n",
    "\n",
    "#keep only the articles that we have embeddings for, since we removed some clusters above\n",
    "outletStoryDf = df.loc[df[\"clustNum\"].isin(allClusts), [\"source\", \"clustNum\"]]\n",
    "\n",
    "#now we have each outlet and stories it covered \n",
    "outletStoryDf = outletStoryDf.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03c6f8f3-8504-40a7-9c7b-eeba192c4e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "787it [00:08, 87.49it/s]\n"
     ]
    }
   ],
   "source": [
    "clusteredStories = outletStoryDf.groupby(\"source\").agg(set)\n",
    "clusteredStories[\"covered\"] = 1\n",
    "\n",
    "notCoveredSamples = [] \n",
    "i = 0 \n",
    "for source, currStories in tqdm(clusteredStories.iterrows()): \n",
    "    # we get the stories not covered by this outlet \n",
    "    # simply all stories minus the stories this outlet did cover \n",
    "    currStories = currStories[\"clustNum\"]\n",
    "    notCovered = set(allClusts) - currStories\n",
    "    \n",
    "    #take 1 times as many negative examples as positive \n",
    "    sample = random.sample(list(notCovered), 1 * len(currStories))\n",
    "    notCoveredSamples.append((source, sample)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd9285db-1b46-4a3c-a6b5-f33d6bebc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe from samples of not covered stories \n",
    "notCoveredDf = pd.DataFrame(notCoveredSamples, columns=[\"source\", \"clustNum\"])\n",
    "notCoveredDf[\"covered\"] = 0\n",
    "\n",
    "#get covered/non-covered stories in long form \n",
    "clusteredStories = clusteredStories.reset_index().explode(\"clustNum\")\n",
    "notCoveredDf = notCoveredDf.explode(\"clustNum\") \n",
    "\n",
    "#merge both covered and not covered training examples \n",
    "#a long form dataframe that gives us outlet, story cluster num, covered or not\n",
    "allCoverage = pd.concat([notCoveredDf.reset_index(drop=True), clusteredStories.reset_index(drop=True)],axis=0) \n",
    "\n",
    "# mix up the rows so that we have equal number of pos/neg training examples \n",
    "# we reset index so we can troubleshoot cross val splits later on\n",
    "allCoverage = allCoverage.sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e76f601b-7b84-412e-aeaf-66339f69afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get outlet level information \n",
    "outletDf = df[[\"source\", \"state\"]].drop_duplicates().fillna(\"National\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3b9d7ec-1205-44fe-9cd6-d9265a84406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "allCoverage = pd.merge(allCoverage, outletDf[[\"source\", \"state\"]], on=\"source\", how=\"left\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db627e-c354-4b4b-b744-a14f68967a3f",
   "metadata": {},
   "source": [
    "### beginning of code for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6dd9b3c-ccb1-4b93-9b74-1a51d9ea1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceNum = 2\n",
    "device = torch.device(\"cuda:\" + str(deviceNum) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b5159ee-7873-421c-bdaa-02dc8dd456ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "\n",
    "    def __init__(self, numEmbeddings, embeddingLen, storyDict):\n",
    "        super(BasicModel, self).__init__()\n",
    "        #in our case, we will have STATE embeddings instead of \n",
    "        #outlet embeddings \n",
    "        self.embeddings = nn.Embedding(numEmbeddings, embeddingLen)\n",
    "        self.storyDict = storyDict\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Dropout = nn.Dropout()\n",
    "        \n",
    "        #we do the length of our embeddings (states) in this case \n",
    "        #plus the length of our story embeddings which is 768 \n",
    "        self.linear1 = nn.Linear(embeddingLen + 768, 200)\n",
    "        self.linear2 = nn.Linear(200, 1) \n",
    "    \n",
    "    #input will be the indices of the embeddings \n",
    "    def forward(self, embedIds, storyVecIds):\n",
    "        #these are the outlet embeddings NOT the story embeddings \n",
    "        stateEmbeds = self.embeddings(embedIds) #.view((1, -1))\n",
    "        storyVecs = torch.tensor([self.storyDict[int(clustNum)] for clustNum in storyVecIds], dtype=torch.float32).to(device)\n",
    "        inTens = torch.concat((stateEmbeds, storyVecs), dim=1).to(device)\n",
    "              \n",
    "        out = self.ReLU(self.Dropout(self.linear1(inTens)))\n",
    "        out = self.linear2(out)\n",
    "        probs = self.Sigmoid(out)\n",
    "        return probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a775182e-5d05-475f-87d1-4b5ff8c5533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOTAL_ROWS = 10000\n",
    "trainDf = allCoverage.head(10000) \n",
    "dataset = Dataset.from_pandas(trainDf)\n",
    "#trainDataset, validDataset = random_split(dataset, [.9, .1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "898a3ad0-ed49-490a-9cf8-a83ffefa6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary that gives a unique index for each state \n",
    "states = allCoverage[\"state\"].unique()\n",
    "stateDict = {states[i]:i for i in range(0, len(states))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5668286-a9aa-436d-afee-a581dad65291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validLoader): \n",
    "    #validation loop \n",
    "    allPreds = []\n",
    "    allGts = []\n",
    "    for batch in validLoader: \n",
    "        outletLookups = torch.tensor([outletDict[outlet] for outlet in batch[\"source\"]]).to(device)\n",
    "        preds = model(outletLookups, batch[\"clustNum\"].to(device))\n",
    "        gts = torch.unsqueeze(batch[\"covered\"], dim=1).to(device) \n",
    "        allPreds += preds.detach().squeeze().cpu().tolist()\n",
    "        allGts += gts.detach().squeeze().cpu().tolist()\n",
    "    return f1_score(torch.tensor(allPreds), torch.tensor(allGts)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0aef30a9-6fc8-4d4f-aac3-eefd414b87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "# testing out cross validation\n",
    "\n",
    "validTups = []\n",
    "trainTups = []\n",
    "\n",
    "totalRows = len(trainDf) \n",
    "trainExamples = totalRows * ((K_FOLDS - 1)/ K_FOLDS) \n",
    "kfold = KFold(n_splits=K_FOLDS, shuffle=True)\n",
    "config = {\n",
    "    \"lr\":LR,\n",
    "    \"batchSize\":TRAIN_BATCH_SIZE,\n",
    "    \"numFolds\":K_FOLDS, \n",
    "    \"totalExamples\":totalRows,\n",
    "    \"trainExamples\":trainExamples, \n",
    "    \"loss\":\"Binary Cross Entropy\"\n",
    "}\n",
    "\n",
    "#we also want to calculate how frequently we should be running on the validation set\n",
    "validMultiple = int((trainExamples / TRAIN_BATCH_SIZE) / VALID_COUNT ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee09f066-2a89-4b62-800f-e94ab016a9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:127vs41r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>trainLoss</td><td>█▆█▂▆▁▇</td></tr><tr><td>validF1</td><td>██▆▅▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>trainLoss</td><td>0.70259</td></tr><tr><td>validF1</td><td>0.57341</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">curious-fire-15</strong>: <a href=\"https://wandb.ai/blitt/localNews-notebooks_IRTModelling/runs/127vs41r\" target=\"_blank\">https://wandb.ai/blitt/localNews-notebooks_IRTModelling/runs/127vs41r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/shared/3/projects/newsDiffusion/models/IRTModelling/wandb/run-20230509_013848-127vs41r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:127vs41r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/shared/3/projects/newsDiffusion/models/IRTModelling/wandb/run-20230509_141329-348qphdx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/blitt/localNews-notebooks_IRTModelling/runs/348qphdx\" target=\"_blank\">logical-durian-16</a></strong> to <a href=\"https://wandb.ai/blitt/localNews-notebooks_IRTModelling\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:02<00:00, 24.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:348qphdx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>trainLoss</td><td>▆▇█▃▁</td></tr><tr><td>validF1</td><td>█▄▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>trainLoss</td><td>0.69302</td></tr><tr><td>validF1</td><td>0.49299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">logical-durian-16</strong>: <a href=\"https://wandb.ai/blitt/localNews-notebooks_IRTModelling/runs/348qphdx\" target=\"_blank\">https://wandb.ai/blitt/localNews-notebooks_IRTModelling/runs/348qphdx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/shared/3/projects/newsDiffusion/models/IRTModelling/wandb/run-20230509_141329-348qphdx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:348qphdx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/shared/3/projects/newsDiffusion/models/IRTModelling/wandb/run-20230509_141342-2stb5oy9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/blitt/localNews-notebooks_IRTModelling/runs/2stb5oy9\" target=\"_blank\">dazzling-elevator-17</a></strong> to <a href=\"https://wandb.ai/blitt/localNews-notebooks_IRTModelling\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:02<00:00, 25.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2stb5oy9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>trainLoss</td><td>▁▅█▅▅</td></tr><tr><td>validF1</td><td>▁▇█▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>trainLoss</td><td>0.69536</td></tr><tr><td>validF1</td><td>0.50767</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dazzling-elevator-17</strong>: <a href=\"https://wandb.ai/blitt/localNews-notebooks_IRTModelling/runs/2stb5oy9\" target=\"_blank\">https://wandb.ai/blitt/localNews-notebooks_IRTModelling/runs/2stb5oy9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/shared/3/projects/newsDiffusion/models/IRTModelling/wandb/run-20230509_141342-2stb5oy9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2stb5oy9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/shared/3/projects/newsDiffusion/models/IRTModelling/wandb/run-20230509_141400-2dvhyki0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/blitt/localNews-notebooks_IRTModelling/runs/2dvhyki0\" target=\"_blank\">jolly-brook-18</a></strong> to <a href=\"https://wandb.ai/blitt/localNews-notebooks_IRTModelling\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:02<00:00, 25.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for fold, (trainIds, validIds) in enumerate(kfold.split(trainDf)):\n",
    "    \n",
    "    wandb.init(dir=\"/shared/3/projects/newsDiffusion/models/IRTModelling/\",reinit=True, config=config,group=GROUP_NAME)\n",
    "    model = BasicModel(len(states) , 200, storyDict) \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"fold: {fold}\")\n",
    "    \n",
    "    trainDataset = Dataset.from_pandas(trainDf.iloc[trainIds,])\n",
    "    validDataset = Dataset.from_pandas(trainDf.iloc[validIds,])\n",
    "    \n",
    "    trainLoader = DataLoader(trainDataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    validLoader = DataLoader(validDataset, batch_size=VALID_BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # and setup a warmup for the first ~10% steps\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    total_steps = int((len(trainDataset) * EPOCHS) / TRAIN_BATCH_SIZE)\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps - warmup_steps)\n",
    "    \n",
    "    subLossList = []\n",
    "    i = 0 \n",
    "    for batch in tqdm(trainLoader): \n",
    "        model.train()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        #get the outlet ids from the outlet names fed into lookup dictionary  \n",
    "        stateLookups = torch.tensor([stateDict[state] for state in batch[\"state\"]]).to(device)\n",
    "        preds = model(stateLookups, batch[\"clustNum\"].to(device))\n",
    "\n",
    "        #get ground truth labels from the batch \n",
    "        gts = torch.unsqueeze(batch[\"covered\"], dim=1).type(\"torch.FloatTensor\").to(device) \n",
    "\n",
    "        loss = loss_func(preds, gts)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        subLossList.append(loss.detach().item())\n",
    "        if i % validMultiple == 0:\n",
    "            model.eval()\n",
    "            trainLoss = np.mean(subLossList) \n",
    "            validF1 = validate(validLoader)\n",
    "            \n",
    "            #add to dataframe \n",
    "            validTups.append((fold, i, validF1))\n",
    "            trainTups.append((fold, i, trainLoss))\n",
    "            \n",
    "            #log to weights and biases \n",
    "            wandb.log({\"trainLoss\":trainLoss, \"validF1\":validF1}) \n",
    "            subLossList = []\n",
    "            model.train()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146011e8-2597-452d-a5db-56e7588eea59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
