{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch \n",
    "import transformers\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import RobertaTokenizer, PreTrainedTokenizer, DistilBertTokenizer, DistilBertModel, RobertaModel\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, util\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "import random\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "from torch import nn\n",
    "#Build up to SBERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceNum = 0\n",
    "device = torch.device(\"cuda:\" + str(deviceNum) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GRAD_ACC = 6\n",
    "EPOCHS = 1\n",
    "FOLDS = 5\n",
    "SEED = 85\n",
    "BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Training/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training data \n",
    "df = pd.read_csv(\"/shared/3/projects/newsDiffusion/data/processed/translated_288_96.tsv\", sep=\"\\t\", nrows=100)\n",
    "\n",
    "#put ground truth values into a list \n",
    "df[\"ground_truth\"] = df['Overall']\n",
    "\n",
    "#quirk from reusing code \n",
    "leanDf = df\n",
    "\n",
    "#rescale data from (0, 4): (0, 1)\n",
    "leanDf[\"ground_truth\"] = 1 - ((leanDf[\"ground_truth\"] - 1) / 3)\n",
    "\n",
    "#reset index so it is contiguous set of numbers \n",
    "leanDf = leanDf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get test data \n",
    "#this is the test data that has already had the \n",
    "#title concatenated and the head + tail merged\n",
    "testDf = pd.read_csv(\"/shared/3/projects/newsDiffusion/data/processed/enTestTranslated_288_96.tsv\", sep=\"\\t\")\n",
    "\n",
    "testDf[\"ground_truth\"] = testDf[\"Overall\"]\n",
    "\n",
    "#rescale ground truth to be between zero and one\n",
    "#testDf[\"ground_truth\"] = 1 - ((testDf[\"ground_truth\"] - 1) / 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([96.,  7.,  0., 30.,  1.,  8., 34.,  2.,  8., 50.]),\n",
       " array([1. , 1.3, 1.6, 1.9, 2.2, 2.5, 2.8, 3.1, 3.4, 3.7, 4. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(testDf[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi encoder model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiModel(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(BiModel,self).__init__()\n",
    "        print(torch.seed())\n",
    "        self.model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2').to(device).train()\n",
    "        self.cos = torch.nn.CosineSimilarity(dim=1, eps=1e-4)\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask): \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask): \n",
    "        \n",
    "        input_ids_a = input_ids[0].to(device)\n",
    "        input_ids_b = input_ids[1].to(device)\n",
    "        attention_a = attention_mask[0].to(device)\n",
    "        attention_b = attention_mask[1].to(device)\n",
    "        \n",
    "        #encode sentence and get mean pooled sentence representation \n",
    "        encoding1 = self.model(input_ids_a, attention_mask=attention_a)[0] #all token embeddings\n",
    "        encoding2 = self.model(input_ids_b, attention_mask=attention_b)[0]\n",
    "        \n",
    "        meanPooled1 = self.mean_pooling(encoding1, attention_a)\n",
    "        meanPooled2 = self.mean_pooling(encoding2, attention_b)\n",
    "        \n",
    "        pred = self.cos(meanPooled1, meanPooled2)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-encoder training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBi(trainDataset): \n",
    "    print(torch.seed())\n",
    "    model = BiModel().to(device)\n",
    "    \n",
    "    # we would initialize everything first\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=2e-6)\n",
    "    \n",
    "    # and setup a warmup for the first ~10% steps\n",
    "    total_steps = int(len(trainDataset) / BATCH_SIZE)*EPOCHS\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps - warmup_steps)\n",
    "\n",
    "    loss_func = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"EPOCH: \" + str(epoch))\n",
    "        \n",
    "        model.train()  # make sure model is in training mode\n",
    "\n",
    "        for batch in tqdm(trainLoader):\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            input_ids = [batch[\"text1Merged_input_ids\"], batch[\"text2Merged_input_ids\"]]\n",
    "            attention_masks = [batch[\"text1Merged_attention_mask\"], batch[\"text2Merged_attention_mask\"]]\n",
    "            pred = model(input_ids, attention_masks)\n",
    "            \n",
    "            gt = batch[\"ground_truth\"].to(device)\n",
    "            loss = loss_func(pred, gt)\n",
    "            \n",
    "            # using loss, calculate gradients and then optimize\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in test dataset for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in our test data \n",
    "testDataset = Dataset.from_pandas(testDf[[\"text1Merged\", \"text2Merged\"]])\n",
    "\n",
    "#tokenizer\n",
    "biTokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "all_cols = []\n",
    "for part in [\"text1Merged\", \"text2Merged\"]: \n",
    "    #tokenizes each row of the dataset and gives us back tuple of lists \n",
    "    testDataset = testDataset.map(lambda x: biTokenizer(x[part], max_length=384, padding=\"max_length\", truncation=True))\n",
    "\n",
    "    for col in ['input_ids', 'attention_mask']: \n",
    "        testDataset = testDataset.rename_column(col, part+'_'+col)\n",
    "        all_cols.append(part+'_'+col)\n",
    "\n",
    "testDataset.set_format(type='torch', columns=all_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07511eb5bf164de3beddf10c2c871185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8898e2ca84496e87455f53eef254d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def testModel(trainedModel, testDataset): \n",
    "    testLoader = torch.utils.data.DataLoader(testDataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    simList = []\n",
    "    for i, batch in tqdm(enumerate(testLoader)): \n",
    "        ids = [batch[\"text1Merged_input_ids\"],batch[\"text2Merged_input_ids\"]]\n",
    "        masks = [batch[\"text1Merged_attention_mask\"],batch[\"text2Merged_attention_mask\"]]\n",
    "        sim = trainedModel(ids, masks)\n",
    "        simList += sim.detach().cpu().tolist()\n",
    "    \n",
    "    \n",
    "    testDf[\"sims\"] = simList\n",
    "    testDf[\"scaledSims\"] = (3*(1-testDf[\"sims\"])) + 1\n",
    "    \n",
    "    corrMat = np.corrcoef(testDf[\"ground_truth\"], testDf[\"scaledSims\"])\n",
    "    corr = corrMat[0, 1]\n",
    "    print(corr)\n",
    "    return [corr, testDf, corrMat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model over 5 seeds, save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df len: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ff94dc8b4846e89ef8942dd061360e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce8ea1b386b4f07b2aca89a445a5e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9985177244475411874\n",
      "8478962953745225685\n",
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ed4410f159403e9a815ca6d00de274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352d8aeb8c4647a6b6a1ad2be77775d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8238620611982242\n",
      "Train df len: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb0f17316664b5788d4e3095913d0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854c758018bc48c3bf7dc12e66fd4269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587418569573560914\n",
      "472571253582557449\n",
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2458c915d31e4138a99710fb98ce8663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c216d0a8a34c848594fcbece4c05fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8254846657048992\n",
      "Train df len: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8499cca73ddf450798898ac435fc4e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797c8b48c85545428a0fbe1100e3cb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14636926035914310847\n",
      "2186919020913630768\n",
      "EPOCH: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742bc8b35d3b4f18bea1fa836067b1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-13-905773ec7ec1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">31</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-11-4247d610762b&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">33</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">trainBi</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/anaconda/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">401</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 398 │   │   │   │   </span>retain_graph=retain_graph,                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 399 │   │   │   │   </span>create_graph=create_graph,                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 400 │   │   │   │   </span>inputs=inputs)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 401 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 402 │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 403 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_hook</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hook):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 404 │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"Registers a backward hook.</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/anaconda/lib/python3.9/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">191</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188 │   # The reason we repeat same the comment below is that</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">189 │   # some Python versions print out the first line of a multi-line function</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">190 │   # calls in the traceback and some print out the last line</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>191 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run th</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">192 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">193 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine </span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">194 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33m<ipython-input-13-905773ec7ec1>\u001b[0m:\u001b[94m31\u001b[0m in \u001b[92m<module>\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33m<ipython-input-11-4247d610762b>\u001b[0m:\u001b[94m33\u001b[0m in \u001b[92mtrainBi\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/anaconda/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m401\u001b[0m in \u001b[92mbackward\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 398 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mretain_graph=retain_graph,                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 399 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 400 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs)                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 401 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 402 \u001b[0m\u001b[2m│   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 403 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_hook\u001b[0m(\u001b[96mself\u001b[0m, hook):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 404 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\"\"Registers a backward hook.\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/anaconda/lib/python3.9/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m191\u001b[0m in \u001b[92mbackward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m190 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m191 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run th\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m192 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m193 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine \u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m194 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_OUTPUT_STEM = \"/shared/3/projects/newsDiffusion/models/2.0-biModelAblation/finalModel/\"\n",
    "\n",
    "seedList = [85, 92, 200, 135, 60]\n",
    "finalCorrs = {}\n",
    "\n",
    "for seed in seedList:\n",
    "    \n",
    "    #set seeds \n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #get tokenizer. This is done in the loop so we have random ordering\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    biTokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "    print(\"Train df len: \" +  str(len(leanDf)))\n",
    "\n",
    "    trainDataset = Dataset.from_pandas(leanDf)\n",
    "\n",
    "    all_cols = [\"ground_truth\"]\n",
    "    #NOTE: here we use the merged text\n",
    "    for part in [\"text1Merged\", \"text2Merged\"]: \n",
    "        #tokenizes each row of the dataset and gives us back tuple of lists \n",
    "        trainDataset = trainDataset.map(lambda x: biTokenizer(x[part], max_length=384, padding=\"max_length\", truncation=True))\n",
    "\n",
    "        for col in ['input_ids', 'attention_mask']: \n",
    "            trainDataset = trainDataset.rename_column(col, part+'_'+col)\n",
    "            all_cols.append(part+'_'+col)\n",
    "\n",
    "    trainDataset.set_format(type='torch', columns=all_cols)\n",
    "    trainedModel = trainBi(trainDataset)\n",
    "    finalCorrs[seed] = testModel(trainedModel, testDataset)[0]\n",
    "    \n",
    "    #save this trained model. We will use the best one in the pipeline \n",
    "    torch.save(trainedModel.state_dict(), MODEL_OUTPUT_STEM + str(seed) + \"/state_dict.tar\")\n",
    "    \n",
    "    #just for memory purposes \n",
    "    del trainedModel \n",
    "    del trainDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-14-16bccb9d9752&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'trainedModel'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33m<ipython-input-14-16bccb9d9752>\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'trainedModel'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr, testDf, corrMat = testModel(trainedModel, testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf[\"simsScaled\"] = (3*(1-testDf[\"sims\"])) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.85752434],\n",
       "       [0.85752434, 1.        ]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(testDf[\"simsScaled\"], testDf[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to an output folder \n",
    "RESULTS_PATH = \"/shared/3/projects/newsDiffusion/models/2.0-biModelAblation/finalModel/\"\n",
    "\n",
    "import pickle \n",
    "with open(RESULTS_PATH + \"modelResults.pkl\", \"wb\") as f: \n",
    "    pickle.dump(finalCorrs, f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
