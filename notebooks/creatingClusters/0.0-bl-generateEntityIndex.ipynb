{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox for getting article pair similarity after filtering by named entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial import distance \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: very important, which entity categories to keep \n",
    "#article showing all entity types below\n",
    "# https://www.kaggle.com/code/curiousprogrammer/entity-extraction-and-classification-using-spacy\n",
    "TO_KEEP = [\"org\",\"event\", \"person\", \"work_of_art\", \"product\"]\n",
    "CLUSTER_CUTOFF = [2, 2000]\n",
    "#for testing \n",
    "NROWS = 20000\n",
    "OUT_PATH = \"/shared/3/projects/newsDiffusion/data/interim/NEREmbedding/embeddingClusterList_NewPiplineTest.tsv\"\n",
    "SIM_THRESH = .8 \n",
    "INVERTED_OUT_PATH = \"/shared/3/projects/newsDiffusion/data/interim/NEREmbedding/invertedEntityIndex2_2000.pkl\"\n",
    "\n",
    "EMBEDS_PATH = \"/shared/3/projects/newsDiffusion/data/processed/articleEmbeddings/embeddings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in main data source \n",
    "#we don't want to use \"content\", because it takes up a lot of space and\n",
    "#we have already embedded the content. Can always merge back in later so long as we \n",
    "#keep the \"key\" column\n",
    "LOAD_COLS = list(pd.read_csv(\"/shared/3/projects/newsDiffusion/data/processed/newsData/fullDataWithNERCleaned.tsv\", \\\n",
    "                     nrows = 1, sep=\"\\t\").columns)\n",
    "LOAD_COLS.remove(\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading news data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-f06f67f50df5>:3: DtypeWarning: Columns (3,4,11,12,14,15,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/shared/3/projects/newsDiffusion/data/processed/newsData/fullDataWithNERCleaned.tsv\",\\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTODO: rewrite this so that we don\\'t do any merging. We just use an embedding dictionary loaded from a pickled object  \\n\\n#load in Embeddings, which haven\\'t been merged yet\\n#we merge them in this step because they are very large and don\\'t\\n#want to write them to disk again if we can help it\\nprint(\"loading embeddings\")\\nembeddingsDf = pd.read_csv(\"/shared/3/projects/newsDiffusion/data/interim/NEREmbedding/embeddingsKeys.tsv\", sep=\"\\t\", names=[\"key\", \"embedding\"], converters={\"embedding\":lambda x: np.array(x.strip(\"[]\").split(\",\"), dtype=float)})\\n\\nprint(\"merging embeddings\")\\ndf = pd.merge(df, embeddingsDf, how=\"inner\", on=\"key\")\\ndf.dropna(subset=[\"key\", \"embedding\"])\\nprint(str(len(df)) + \" rows after merging, dropping na keys, embeddings\")\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in main data source \n",
    "print(\"loading news data\")\n",
    "df = pd.read_csv(\"/shared/3/projects/newsDiffusion/data/processed/newsData/fullDataWithNERCleaned.tsv\",\\\n",
    "                 sep=\"\\t\", usecols = LOAD_COLS)\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"mixed\")\n",
    "\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "\n",
    "\"\"\"\n",
    "TODO: rewrite this so that we don't do any merging. We just use an embedding dictionary loaded from a pickled object  \n",
    "\n",
    "#load in Embeddings, which haven't been merged yet\n",
    "#we merge them in this step because they are very large and don't\n",
    "#want to write them to disk again if we can help it\n",
    "print(\"loading embeddings\")\n",
    "embeddingsDf = pd.read_csv(\"/shared/3/projects/newsDiffusion/data/interim/NEREmbedding/embeddingsKeys.tsv\", sep=\"\\t\", names=[\"key\", \"embedding\"], converters={\"embedding\":lambda x: np.array(x.strip(\"[]\").split(\",\"), dtype=float)})\n",
    "\n",
    "print(\"merging embeddings\")\n",
    "df = pd.merge(df, embeddingsDf, how=\"inner\", on=\"key\")\n",
    "df.dropna(subset=[\"key\", \"embedding\"])\n",
    "print(str(len(df)) + \" rows after merging, dropping na keys, embeddings\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstLocal = min(df.loc[(df[\"national\"] == False), \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4228898 rows in overlapping period\n"
     ]
    }
   ],
   "source": [
    "#filter to only the overlapping sections \n",
    "#should give us everything from April 1, 2020 - December 31, 2021 \n",
    "df = df[df[\"date\"] >= firstLocal]\n",
    "\n",
    "#filter so we only use the part of 2020 where we have overlap \n",
    "#df = df[df[\"year\"] == 2020]\n",
    "\n",
    "#get length of new rows \n",
    "print(str(len(df)) + \" rows in overlapping period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date range: \n",
      "2021-12-31 00:00:00\n",
      "2020-04-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"date range: \")\n",
    "print(max(pd.to_datetime(df[\"date\"])))\n",
    "print(min(pd.to_datetime(df[\"date\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-9fd2242309ca>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  leanDf[\"NamedEntities\"] = leanDf[\"NamedEntities\"].apply(parseList)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed\n",
      "0 NA values in Named Entities column\n",
      "Filling with '' instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-9fd2242309ca>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  leanDf[\"NamedEntities\"] = leanDf[\"NamedEntities\"].fillna(\"\")\n"
     ]
    }
   ],
   "source": [
    "#NOTE: used to have embeddings here, but don't need that anymore with current method \n",
    "leanDf = df[[\"key\", \"NamedEntities\"]]\n",
    "\n",
    "print(\"parsing\")\n",
    "\n",
    "def cleanList(inList): \n",
    "    return [str(re.sub(\"[^a-zA-Z0-9 ]\", \"\", item).lower()) for item in inList]\n",
    "\n",
    "def parseList(inStr): \n",
    "    split = inStr.split(\"\\'), (\\'\")\n",
    "    return [cleanList(item.split(\"', '\")) for item in split]\n",
    "\n",
    "#parse topics from string to actual list of tuples \n",
    "leanDf[\"NamedEntities\"] = leanDf[\"NamedEntities\"].apply(parseList)\n",
    "\n",
    "print(\"parsed\")\n",
    "\n",
    "#test out idea for creating reverse mapping \n",
    "#how many na vals do we have in \"NamedEntities\"? \n",
    "print(str(sum(leanDf[\"NamedEntities\"].isna())) + \" NA values in Named Entities column\")\n",
    "print(\"Filling with '' instead\")\n",
    "leanDf[\"NamedEntities\"] = leanDf[\"NamedEntities\"].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we see below that we have things like \"date: week\" as named entities. This must be addressed somewhere "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploding #1\n",
      "splitting entity, type\n",
      "filtering by entity type, grouping\n"
     ]
    }
   ],
   "source": [
    "#bring each tuple into its own row \n",
    "print(\"exploding #1\")\n",
    "invertedDf = leanDf.explode(\"NamedEntities\")\n",
    "\n",
    "#bring each tuple entry into its own column \n",
    "#split ent_type, entity pairs to columns \n",
    "print(\"splitting entity, type\")\n",
    "invertedDf[[\"ent_type\",\"entity\"]] = pd.DataFrame(invertedDf[\"NamedEntities\"].tolist(), index=invertedDf.index)\n",
    "\n",
    "#remove occurences where we double count an entity for the same article \n",
    "invertedDf = invertedDf.drop_duplicates(subset=[\"key\", \"ent_type\", \"entity\"])\n",
    "\n",
    "print(\"filtering by entity type, grouping\")\n",
    "#keep only the entity types that may be interesting \n",
    "invertedDf = invertedDf[invertedDf[\"ent_type\"].isin(TO_KEEP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7436473 rows in entity-grouped df\n"
     ]
    }
   ],
   "source": [
    "#group articles by their named entities  \n",
    "invertedDf = invertedDf[[\"ent_type\", \"entity\", \"key\"]].groupby(by=[\"ent_type\", \"entity\"]).agg(list)\n",
    "\n",
    "print(str(len(invertedDf)) + \" rows in entity-grouped df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ent_type</th>\n",
       "      <th>entity</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">event</th>\n",
       "      <th></th>\n",
       "      <td>[3731573]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730am</th>\n",
       "      <td>[4353276]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arab</th>\n",
       "      <td>[4933735, 4933736]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duty</th>\n",
       "      <td>[1622968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immunoprivilege</th>\n",
       "      <td>[2500373]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             key\n",
       "ent_type entity                                 \n",
       "event                                  [3731573]\n",
       "           730am                       [4353276]\n",
       "           arab               [4933735, 4933736]\n",
       "           duty                        [1622968]\n",
       "           immunoprivilege             [2500373]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invertedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "invertedDf[\"numArticles\"] = invertedDf[\"key\"].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7436473"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(invertedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVERTED_OUT_PATH = \"/shared/3/projects/newsDiffusion/data/interim/NEREmbedding/invertedEntityIndex.pkl\"\n",
    "\n",
    "#NOTE: start again here\n",
    "# export our named entity inverted index so that we can do analysis as we need to in another script \n",
    "invertedDf.reset_index().to_pickle(INVERTED_OUT_PATH, compression=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
